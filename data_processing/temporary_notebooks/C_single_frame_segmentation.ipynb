{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd73e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time \n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import importlib\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import config\n",
    "\n",
    "from model_training import training_data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb47ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8dcd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIRS_1 = [\n",
    "    '31_03_21__318__3or4_people/1/006__11_44_59',\n",
    "    '31_03_21__318__3or4_people/1/007__11_48_59',\n",
    "    '31_03_21__318__3or4_people/1/008__11_52_59',\n",
    "    '31_03_21__318__3or4_people/1/009__11_57_00',\n",
    "     ]\n",
    "\n",
    "TRAINING_DIRS_2 = [\n",
    "    '31_03_21__318__3or4_people/2/000__14_15_19',\n",
    "    '31_03_21__318__3or4_people/2/001__14_19_19',\n",
    "    '31_03_21__318__3or4_people/2/002__14_23_19',\n",
    "    '31_03_21__318__3or4_people/2/003__14_27_20',\n",
    "    '31_03_21__318__3or4_people/2/004__14_31_20',\n",
    "\n",
    "    '31_03_21__318__3or4_people/2/010__14_55_20',\n",
    "    '31_03_21__318__3or4_people/2/011__14_59_20',\n",
    "    '31_03_21__318__3or4_people/2/012__15_03_21',\n",
    "    '31_03_21__318__3or4_people/2/013__15_07_21',\n",
    "    '31_03_21__318__3or4_people/2/014__15_11_21',\n",
    "    '31_03_21__318__3or4_people/2/015__15_15_21',\n",
    "    '31_03_21__318__3or4_people/2/016__15_19_21',\n",
    "    ]\n",
    "\n",
    "VALIDATION_DIRS_1 = [\n",
    "    '31_03_21__318__3or4_people/2/005__14_35_20',\n",
    "    '31_03_21__318__3or4_people/2/006__14_39_20',\n",
    "    '31_03_21__318__3or4_people/2/007__14_43_20',\n",
    "    '31_03_21__318__3or4_people/2/008__14_47_20',\n",
    "    '31_03_21__318__3or4_people/2/009__14_51_20',\n",
    "]\n",
    "\n",
    "_training_data_1 = training_data_loader.load_data_for_labeled_batches(labeled_batch_dirs=TRAINING_DIRS_1)\n",
    "_training_data_2 = training_data_loader.load_data_for_labeled_batches(labeled_batch_dirs=TRAINING_DIRS_2)\n",
    "_validation_data_1 = training_data_loader.load_data_for_labeled_batches(labeled_batch_dirs=VALIDATION_DIRS_1)\n",
    "\n",
    "augmented_data_training = training_data_loader.AugmentedBatchesTrainingData()\n",
    "augmented_data_training.add_training_batch(_training_data_1)\n",
    "augmented_data_training.add_training_batch(_training_data_2)\n",
    "\n",
    "augmented_data_validation = training_data_loader.AugmentedBatchesTrainingData()\n",
    "augmented_data_validation.add_training_batch(_validation_data_1)\n",
    "\n",
    "\n",
    "\n",
    "# for centre_points in _training_data_2.centre_points:\n",
    "#     if len(centre_points) > 4:\n",
    "#         print(\"Too many people on one frame in annotations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815043b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa1b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66b825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data_training.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d056f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data_validation.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f1dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d0bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce89916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_OUPUT_CLASSES = 2\n",
    "\n",
    "def draw_airbrush_circle(img, centre, radius):\n",
    "    for x in range(max(0, round(centre[0]-radius)), min(img.shape[0], round(centre[0]+radius+1))):\n",
    "        for y in range(max(0, round(centre[1]-radius)), min(img.shape[1], round(centre[1]+radius+1))):\n",
    "            point = (x, y)\n",
    "            distance_to_centre = cv2.norm((centre[0] - x, centre[1] - y))\n",
    "            if distance_to_centre > radius:\n",
    "                continue\n",
    "            #img[point] = max(img[point], 1 - distance_to_centre / radius)\n",
    "            img[point] = 1\n",
    "            \n",
    "\n",
    "def draw_cross(img, centre, cross_width, cross_height):\n",
    "    for x in range(max(0, round(centre[0]) - cross_width), min(img.shape[0], round(centre[0]) + cross_width + 1)):\n",
    "        for y in range(max(0, round(centre[1]) - cross_height), min(img.shape[1], round(centre[1]) + cross_height + 1)):\n",
    "            point = (x, y)\n",
    "            img[point] = 1\n",
    "    \n",
    "    for x in range(max(0, round(centre[0] - cross_height)), min(img.shape[0], round(centre[0] + cross_height + 1))):\n",
    "        for y in range(max(0, round(centre[1] - cross_width)), min(img.shape[1], round(centre[1] + cross_width + 1))):\n",
    "            point = (x, y)\n",
    "            img[point] = 1\n",
    "    \n",
    "    \n",
    "def get_img_reconstructed_from_labels(centre_points):\n",
    "    SCALE_FACTOR = 1  # 5\n",
    "    img_reconstructed_scaled = np.zeros(shape=(config.IR_CAMERA_RESOLUTION[0] * SCALE_FACTOR, \n",
    "                                               config.IR_CAMERA_RESOLUTION[1] * SCALE_FACTOR))\n",
    "\n",
    "    for centre_point in centre_points:\n",
    "        centre_point = centre_point[::-1]  # reversed x and y in \n",
    "\n",
    "        draw_cross(img=img_reconstructed_scaled, \n",
    "                             centre=[p*SCALE_FACTOR for p in centre_point], \n",
    "                             #radius=int(2*SCALE_FACTOR),\n",
    "                   cross_height = 2, cross_width=1)\n",
    "\n",
    "    # plt.imshow(img_reconstructed_scaled)\n",
    "\n",
    "    img_reconstructed_original_size = cv2.resize(\n",
    "        src=img_reconstructed_scaled, \n",
    "        dsize=config.IR_CAMERA_RESOLUTION_XY, \n",
    "        interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    \n",
    "    img_int = (img_reconstructed_original_size * (NUMBER_OF_OUPUT_CLASSES-1)).astype('int')\n",
    "    return img_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d77f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b09c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_training.training_data_loader import AugmentedBatchesTrainingData\n",
    "\n",
    "class IrPersonsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, augmented_data: AugmentedBatchesTrainingData, transform=None):\n",
    "        self.augmented_data = AugmentedBatchesTrainingData\n",
    "        self.transform = transform\n",
    "        self._index_to_batch_and_subindex_map = {}\n",
    "        \n",
    "        i = 0\n",
    "        for batch in augmented_data.batches:\n",
    "            for j in range(len(batch.raw_ir_data)):\n",
    "                self._index_to_batch_and_subindex_map[i] = (batch, j) \n",
    "                i += 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len (self._index_to_batch_and_subindex_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            raise Exception(\"Not supported\")\n",
    "            #idx = idx.tolist()\n",
    "\n",
    "        batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "        frame = batch.normalized_ir_data[subindex][np.newaxis, :, :]\n",
    "        \n",
    "        batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "        centre_points = batch.centre_points[subindex]\n",
    "        img_reconstructed = get_img_reconstructed_from_labels(centre_points)\n",
    "        img_reconstructed_3d = img_reconstructed\n",
    "        \n",
    "        return frame, img_reconstructed_3d\n",
    "\n",
    "    def get_number_of_persons_for_frame(self, idx):\n",
    "        batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "        return len(batch.centre_points[subindex])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "training_dataset = IrPersonsDataset(augmented_data_training)\n",
    "validation_dataset = IrPersonsDataset(augmented_data_validation)\n",
    "\n",
    "\n",
    "# it makes no sense to split all data, as most of the frames are almost identical\n",
    "# training_dataset, validation_dataset = torch.utils.data.random_split(all_data_dataset, [training_data_len, validation_data_len])\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(training_dataset, batch_size=1, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "print(len(trainloader))\n",
    "print(len(valloader))\n",
    "\n",
    "\n",
    "\n",
    "train_d1 = trainloader\n",
    "\n",
    "\n",
    "xb, yb = next(iter(train_d1))\n",
    "xb.shape, yb.shape\n",
    "\n",
    "\n",
    "plt.imshow(yb[0].numpy().squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303e475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b164245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19582ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139112e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786773ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = self.contract_block(in_channels, 32, 5, 2)\n",
    "        self.conv2 = self.contract_block(32, 64, 3, 1)\n",
    "        #self.conv3 = self.contract_block(64, 128, 3, 1)\n",
    "\n",
    "        #self.upconv3 = self.expand_block(128, 64, 3, 1)\n",
    "        self.upconv2 = self.expand_block(64, 32, 3, 1)\n",
    "        self.upconv1 = self.expand_block(32*2, out_channels, 3, 1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # downsampling part\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        #conv3 = self.conv3(conv2)\n",
    "\n",
    "        #upconv3 = self.upconv3(conv3)\n",
    "\n",
    "        #upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n",
    "        upconv2 = self.upconv2(conv2)\n",
    "        \n",
    "        upconv1 = self.upconv1(torch.cat([upconv2, conv1], 1))\n",
    "\n",
    "        return upconv1\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        contract = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "                                 )\n",
    "\n",
    "        return contract\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1) \n",
    "                            )\n",
    "        return expand\n",
    "    \n",
    "    \n",
    "unet = UNET(1, 2).double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=unet\n",
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "# params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=unet\n",
    "# sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eaaac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705738a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs=1):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss, valid_loss = [], []\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set trainind mode = true\n",
    "                dataloader = train_dl\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = valid_dl\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "            step = 0\n",
    "\n",
    "            # iterate over data\n",
    "            for x, y in dataloader:\n",
    "                step += 1\n",
    "\n",
    "                # forward pass\n",
    "                if phase == 'train':\n",
    "                    # zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = loss_fn(outputs, y)\n",
    "\n",
    "                    # the backward pass frees the graph memory, so there is no \n",
    "                    # need for torch.no_grad in this training pass\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # scheduler.step()\n",
    "        \n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(x)\n",
    "                        loss = loss_fn(outputs, y.long())\n",
    "\n",
    "                # stats - whatever is the phase\n",
    "                #acc = acc_fn(outputs, y)\n",
    "\n",
    "                #running_acc  += acc*dataloader.batch_size\n",
    "                #running_loss += loss*dataloader.batch_size \n",
    "                running_loss += loss.item()\n",
    "\n",
    "                #time.sleep(0.1)\n",
    "                \n",
    "\n",
    "                if step % 100 == 0:\n",
    "                    # clear_output(wait=True)\n",
    "                    # print('Current step: {}  Loss: {}  Acc: {}'.format(step, loss, acc))\n",
    "                    print('Current step: {}  Loss: {}'.format(step, loss))\n",
    "                    # print(torch.cuda.memory_summary())\n",
    "                          \n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            # epoch_acc = running_acc / len(dataloader.dataset)\n",
    "\n",
    "            #print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "                          \n",
    "            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n",
    "            \n",
    "            \n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "    \n",
    "    return train_loss, valid_loss    \n",
    "\n",
    "\n",
    "def acc_metric(predb, yb):\n",
    "    return (predb.argmax(dim=1) == yb.float().mean())\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(unet.parameters(), lr=0.01)\n",
    "train_loss, valid_loss = train(model=unet, train_dl=train_d1, valid_dl=valloader, \n",
    "                               loss_fn=loss_fn, optimizer=opt, acc_fn=acc_metric, epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet3 = unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187311e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(22):\n",
    "    xb, yb = next(iter(valloader))\n",
    "    #xb, yb = next(iter(train_d1))\n",
    "    \n",
    "    xb.shape, yb.shape\n",
    "\n",
    "\n",
    "    plt.imshow(xb[0].numpy().squeeze())\n",
    "    plt.show()\n",
    "    plt.imshow(yb[0].numpy().squeeze())\n",
    "    plt.show()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = unet(xb)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(1, NUMBER_OF_OUPUT_CLASSES):\n",
    "        plt.imshow(outputs[0,i])\n",
    "        plt.show()\n",
    "    print(\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30bb17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6afac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf30fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be3f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af76f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018bc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4748f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d4fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda5638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1940c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727eeb75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ccb043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fcb860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01cd117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_training.training_data_loader import AugmentedBatchesTrainingData\n",
    "\n",
    "class IrPersonsDatasetAfterUnet(torch.utils.data.Dataset):\n",
    "    def __init__(self, unet_model, unet_dataset):\n",
    "        self.unet_model = unet_model\n",
    "        self._unet_outputs = {}\n",
    "        self._unet_dataset = unet_dataset\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len (self._unet_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx not in self._unet_outputs:\n",
    "            \n",
    "            unet_input, _ = self._unet_dataset[idx]\n",
    "            unet_input = unet_input[np.newaxis, :, :, :]\n",
    "            unet_input = torch.from_numpy(unet_input)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.unet_model(unet_input)\n",
    "            unet_output = outputs[0,1][np.newaxis, :, :]\n",
    "            self._unet_outputs[idx] = unet_output\n",
    "        \n",
    "        return self._unet_outputs[idx], self._unet_dataset.get_number_of_persons_for_frame(idx)\n",
    "\n",
    "    \n",
    "    \n",
    "training_datase_after_unet = IrPersonsDatasetAfterUnet(unet, training_dataset)\n",
    "validation_dataset_after_unet = IrPersonsDatasetAfterUnet(unet, validation_dataset)\n",
    "\n",
    "# it makes no sense to split all data, as most of the frames are almost identical\n",
    "# training_dataset, validation_dataset = torch.utils.data.random_split(all_data_dataset, [training_data_len, validation_data_len])\n",
    "\n",
    "trainloader_after_unet = torch.utils.data.DataLoader(training_datase_after_unet, batch_size=16, shuffle=True)\n",
    "valloader_after_unet = torch.utils.data.DataLoader(validation_dataset_after_unet, batch_size=1, shuffle=True)\n",
    "\n",
    "print(len(trainloader_after_unet))\n",
    "print(len(valloader_after_unet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fdfc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(valloader_after_unet))\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc2cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PEOPLE_TO_COUNT = 5\n",
    "\n",
    "class ModelSingleFrame1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        conv2_out_channels = 32\n",
    "        l1_in_features = config.IR_CAMERA_RESOLUTION_X * config.IR_CAMERA_RESOLUTION_Y // ((2*2)**2) \\\n",
    "            * conv2_out_channels\n",
    "        l1_out_features = 128\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=conv2_out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.l1 = nn.Linear(in_features=l1_in_features, out_features=l1_out_features)\n",
    "        self.l2 = nn.Linear(in_features=l1_out_features, out_features=MAX_PEOPLE_TO_COUNT+1)\n",
    "        \n",
    "        self.l1_in_features = l1_in_features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(-1, self.l1_in_features)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = F.log_softmax(self.l2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "model_conv_1 = ModelSingleFrame1()\n",
    "model = model_conv_1.double()\n",
    "\n",
    "\n",
    "\n",
    "dataiter = iter(trainloader_after_unet)\n",
    "unet_frmaes, labels = dataiter.next()\n",
    "print(unet_frmaes.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "ir_frame_normalized_0 = unet_frmaes[0].numpy().squeeze()\n",
    "plt.imshow(ir_frame_normalized_0)\n",
    "print(f'Persons: {labels[0]}')\n",
    "\n",
    "#print(ir_frame_normalized_0)\n",
    "#print(np.min(ir_frame_normalized_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3970b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e0f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader_after_unet))\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c4239",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "time0 = time.time()\n",
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader_after_unet:    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        #print(max(labels))\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader_after_unet)))\n",
    "        print(\"\\nTraining Time (in minutes) =\",(time.time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4876f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = 0\n",
    "tested_frames = 0\n",
    "number_of_frames_with_n_persons = {}\n",
    "number_of_frames_with_n_persons_predicted_correctly = {}\n",
    "\n",
    "for frame, labels in valloader_after_unet:\n",
    "    for i in range(len(labels)):\n",
    "        with torch.no_grad():\n",
    "            logps = model(frame)\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    \n",
    "    number_of_frames_with_n_persons[pred_label] = \\\n",
    "        number_of_frames_with_n_persons.get(pred_label, 0) + 1\n",
    "    \n",
    "    if true_label == pred_label:\n",
    "        correct_count += 1\n",
    "        number_of_frames_with_n_persons_predicted_correctly[pred_label] = \\\n",
    "            number_of_frames_with_n_persons_predicted_correctly.get(pred_label, 0) + 1\n",
    "    \n",
    "    tested_frames += 1\n",
    "    \n",
    "\n",
    "print(f\"Number of tested frames: {tested_frames}\")\n",
    "print(f\"Model Accuracy = {correct_count / tested_frames}\")\n",
    "print('Predicted:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons.items()]))\n",
    "print('Predicted correctly:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons_predicted_correctly.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cdcc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fe3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc2dc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873fb023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
