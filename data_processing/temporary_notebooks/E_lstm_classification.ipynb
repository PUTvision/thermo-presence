{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd73e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time \n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import importlib\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import config\n",
    "\n",
    "from model_training import training_data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb47ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8dcd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIRS_1 = [\n",
    "    '31_03_21__318__3or4_people/1/006__11_44_59',\n",
    "    '31_03_21__318__3or4_people/1/007__11_48_59',\n",
    "    '31_03_21__318__3or4_people/1/008__11_52_59',\n",
    "    '31_03_21__318__3or4_people/1/009__11_57_00',\n",
    "     ]\n",
    "\n",
    "TRAINING_DIRS_2 = [\n",
    "    '31_03_21__318__3or4_people/2/000__14_15_19',\n",
    "    '31_03_21__318__3or4_people/2/001__14_19_19',\n",
    "    '31_03_21__318__3or4_people/2/002__14_23_19',\n",
    "    '31_03_21__318__3or4_people/2/003__14_27_20',\n",
    "    '31_03_21__318__3or4_people/2/004__14_31_20',\n",
    "    '31_03_21__318__3or4_people/2/005__14_35_20',\n",
    "    '31_03_21__318__3or4_people/2/006__14_39_20',\n",
    "    '31_03_21__318__3or4_people/2/007__14_43_20',\n",
    "    '31_03_21__318__3or4_people/2/008__14_47_20',\n",
    "    '31_03_21__318__3or4_people/2/009__14_51_20',\n",
    "    '31_03_21__318__3or4_people/2/010__14_55_20',\n",
    "    '31_03_21__318__3or4_people/2/011__14_59_20',\n",
    "    '31_03_21__318__3or4_people/2/012__15_03_21',\n",
    "    ]\n",
    "\n",
    "VALIDATION_DIRS_1 = [\n",
    "    '31_03_21__318__3or4_people/2/013__15_07_21',\n",
    "    '31_03_21__318__3or4_people/2/014__15_11_21',\n",
    "    '31_03_21__318__3or4_people/2/015__15_15_21',\n",
    "    '31_03_21__318__3or4_people/2/016__15_19_21',\n",
    "]\n",
    "\n",
    "_training_data_1 = training_data_loader.load_data_for_labeled_batches(labeled_batch_dirs=TRAINING_DIRS_1)\n",
    "_training_data_2 = training_data_loader.load_data_for_labeled_batches(labeled_batch_dirs=TRAINING_DIRS_2)\n",
    "_validation_data_1 = training_data_loader.load_data_for_labeled_batches(labeled_batch_dirs=VALIDATION_DIRS_1)\n",
    "\n",
    "augmented_data_training = training_data_loader.AugmentedBatchesTrainingData()\n",
    "augmented_data_training.add_training_batch(_training_data_1)\n",
    "augmented_data_training.add_training_batch(_training_data_2)\n",
    "\n",
    "augmented_data_validation = training_data_loader.AugmentedBatchesTrainingData()\n",
    "augmented_data_validation.add_training_batch(_validation_data_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815043b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data_training.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d056f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data_validation.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b09c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PEOPLE_TO_COUNT = 4\n",
    "l1_out_features = 128\n",
    "conv2_out_channels = 32\n",
    "l1_in_features = config.IR_CAMERA_RESOLUTION_X * config.IR_CAMERA_RESOLUTION_Y // ((2*2)**2) \\\n",
    "    * conv2_out_channels\n",
    "    \n",
    "    \n",
    "class ModelSingleFrameCnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=conv2_out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(-1, l1_in_features)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "class ModelSingleFrame1(nn.Module):\n",
    "    def __init__(self, cnn_model):\n",
    "        super(ModelSingleFrame1, self).__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.l1 = nn.Linear(in_features=l1_in_features, out_features=l1_out_features)\n",
    "        self.l2 = nn.Linear(in_features=l1_out_features, out_features=MAX_PEOPLE_TO_COUNT+1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_model(x)\n",
    "        \n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = F.log_softmax(self.l2(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "cnn_model = ModelSingleFrameCnn().double()  \n",
    "model = ModelSingleFrame1(cnn_model).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea88a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_training.training_data_loader import AugmentedBatchesTrainingData\n",
    "\n",
    "\n",
    "class IrPersonsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, augmented_data: AugmentedBatchesTrainingData, transform=None):\n",
    "        self.augmented_data = AugmentedBatchesTrainingData\n",
    "        self.transform = transform\n",
    "        self._index_to_batch_and_subindex_map = {}\n",
    "        \n",
    "        i = 0\n",
    "        for batch in augmented_data.batches:\n",
    "            for j in range(len(batch.raw_ir_data)):\n",
    "                self._index_to_batch_and_subindex_map[i] = (batch, j) \n",
    "                i += 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len (self._index_to_batch_and_subindex_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            raise Exception(\"Not supported\")\n",
    "            #idx = idx.tolist()\n",
    "\n",
    "        batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "        frame = batch.normalized_ir_data[subindex][np.newaxis, :, :]\n",
    "        return frame, len(batch.centre_points[subindex])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e58912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = IrPersonsDataset(augmented_data_training)\n",
    "validation_dataset = IrPersonsDataset(augmented_data_validation)\n",
    "\n",
    "\n",
    "# it makes no sense to split all data, as most of the frames are almost identical\n",
    "# training_dataset, validation_dataset = torch.utils.data.random_split(all_data_dataset, [training_data_len, validation_data_len])\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "print(len(trainloader))\n",
    "print(len(valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "ir_frmaes, labels = dataiter.next()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(ir_frmaes.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "ir_frame_normalized_0 = ir_frmaes[0].numpy().squeeze()\n",
    "plt.imshow(ir_frame_normalized_0)\n",
    "print(f'Persons: {labels[0]}')\n",
    "\n",
    "#print(ir_frame_normalized_0)\n",
    "#print(np.min(ir_frame_normalized_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54d971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf31c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ad9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bead8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e18fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "time0 = time.time()\n",
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "        print(\"\\nTraining Time (in minutes) =\",(time.time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b132d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a085f37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd259a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(valloader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    logps = model(images)\n",
    "    \n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.numpy()[0])\n",
    "\n",
    "\n",
    "plt.imshow(images[0].numpy().squeeze())\n",
    "\n",
    "print(\"Predicted number of persons =\", probab.index(max(probab)))\n",
    "print(probab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = 0\n",
    "tested_frames = 0\n",
    "number_of_frames_with_n_persons = {}\n",
    "number_of_frames_with_n_persons_predicted_correctly = {}\n",
    "\n",
    "for frame, labels in valloader:\n",
    "    for i in range(len(labels)):\n",
    "        with torch.no_grad():\n",
    "            logps = model(frame)\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    \n",
    "    number_of_frames_with_n_persons[pred_label] = \\\n",
    "        number_of_frames_with_n_persons.get(pred_label, 0) + 1\n",
    "    \n",
    "    if true_label == pred_label:\n",
    "        correct_count += 1\n",
    "        number_of_frames_with_n_persons_predicted_correctly[pred_label] = \\\n",
    "            number_of_frames_with_n_persons_predicted_correctly.get(pred_label, 0) + 1\n",
    "    \n",
    "    tested_frames += 1\n",
    "    \n",
    "\n",
    "print(f\"Number of tested frames: {tested_frames}\")\n",
    "print(f\"Model Accuracy = {correct_count / tested_frames}\")\n",
    "print('Predicted:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons.items()]))\n",
    "print('Predicted correctly:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons_predicted_correctly.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2ded1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76986d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ed50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25948f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ec8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_training.training_data_loader import AugmentedBatchesTrainingData\n",
    "\n",
    "\n",
    "class IrPersonsDatasetSeries(torch.utils.data.Dataset):\n",
    "    def __init__(self, augmented_data: AugmentedBatchesTrainingData, transform=None):\n",
    "        self.augmented_data = AugmentedBatchesTrainingData\n",
    "        self.transform = transform\n",
    "        self._index_to_batch_and_subindex_map = {}\n",
    "        \n",
    "        i = 0\n",
    "        for batch in augmented_data.batches:\n",
    "            for j in range(len(batch.raw_ir_data)):\n",
    "                self._index_to_batch_and_subindex_map[i] = (batch, j) \n",
    "                i += 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len (self._index_to_batch_and_subindex_map) - TIME_STEPS\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TIME_STEPS consecutive frames\n",
    "        \n",
    "        real_frame_idx = [idx + i for i in range(TIME_STEPS)]\n",
    "        \n",
    "        frames = [self.get_absolute_frame_data(i) for i in real_frame_idx]  \n",
    "        label = self.get_absolute_frame_label(real_frame_idx[-1])  # ???\n",
    "        \n",
    "        frames_4d = np.stack(frames, axis = 0)\n",
    "        #print(frames_4d.shape)\n",
    "        return frames_4d, label\n",
    "        \n",
    "    def get_absolute_frame_data(self, idx):\n",
    "        batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "        frame = batch.normalized_ir_data[subindex][np.newaxis, :, :]\n",
    "        #print(frame.shape)\n",
    "        return frame\n",
    "    \n",
    "    def get_absolute_frame_label(self, idx):\n",
    "        batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "        frame = batch.normalized_ir_data[subindex][np.newaxis, :, :]\n",
    "        return len(batch.centre_points[subindex])\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "training_dataset_series = IrPersonsDatasetSeries(augmented_data_training)\n",
    "trainloader_series = torch.utils.data.DataLoader(training_dataset_series, batch_size=16, shuffle=True)\n",
    "\n",
    "validation_dataset_series = IrPersonsDatasetSeries(augmented_data_training)\n",
    "valloader_series = torch.utils.data.DataLoader(validation_dataset_series, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "# criterion = nn.NLLLoss()\n",
    "# images, labels = next(iter(trainloader))\n",
    "\n",
    "# print(images.shape)\n",
    "# print(labels)\n",
    "\n",
    "# logps = model(images) #log probabilities\n",
    "# print(logps)\n",
    "# loss = criterion(logps, labels) #calculate the NLL loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b001f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdaae74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.randn([5, 32, 60, 60]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c96562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadaad67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85633c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader_series)\n",
    "frames_series, labels = dataiter.next()\n",
    "\n",
    "plt.imshow(frames_series[0, 0, 0])\n",
    "frames_series.shape  # [batch_size, timesteps, C, H, W]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f062887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_out = cnn_model(frames)\n",
    "\n",
    "# cnn_out.shape  # [batch_size, outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4b173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42f853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2463cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9fc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnLstmModel(nn.Module):\n",
    "    def __init__(self, pretrained_cnn_model):\n",
    "        super(CnnLstmModel, self).__init__()\n",
    "        self.pretrained_cnn_model = pretrained_cnn_model\n",
    "\n",
    "        self.lstm = nn.LSTM(l1_in_features, hidden_size=l1_out_features, num_layers=2, batch_first=True)\n",
    "\n",
    "        \n",
    "        # move l1 to cnn? Or does lstm behaves as l1?\n",
    "#         self.l1 = nn.Linear(in_features=l1_in_features, out_features=l1_out_features)\n",
    "        self.l2 = nn.Linear(in_features=l1_out_features, out_features=MAX_PEOPLE_TO_COUNT+1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "\n",
    "        x = self.pretrained_cnn_model(c_in)\n",
    "        \n",
    "        lstm_in = x.view(batch_size, timesteps, -1)\n",
    "\n",
    "        lstm_out, hidden = self.lstm(lstm_in)\n",
    "        \n",
    "        #print(lstm_out.shape)\n",
    "        lstm_out_last_frame = lstm_out[:,-1,:]\n",
    "        #print(lstm_out_last_frame.shape)\n",
    "        \n",
    "        \n",
    "        x = F.relu(lstm_out_last_frame)\n",
    "        x = F.dropout(x, p=0.2)  # ???\n",
    "        x = F.log_softmax(self.l2(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "cnn_lstm_model = CnnLstmModel(pretrained_cnn_model=cnn_model).double()\n",
    "cnn_lstm_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "frames_series, labels = iter(trainloader_series).next()\n",
    "print(frames_series.shape)\n",
    "\n",
    "out = cnn_lstm_model(frames_series)\n",
    "\n",
    "\n",
    "out.shape\n",
    "out[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f863598a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcc4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "images_series, labels = next(iter(trainloader_series))\n",
    "\n",
    "print(images_series.shape)\n",
    "\n",
    "logps = cnn_lstm_model(images_series) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1ddcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bd1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "time0 = time.time()\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for frame_series, labels in trainloader_series:    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = cnn_lstm_model(frame_series)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "        print(\"\\nTraining Time (in minutes) =\",(time.time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c79d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac16394",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = 0\n",
    "tested_frames = 0\n",
    "number_of_frames_with_n_persons = {}\n",
    "number_of_frames_with_n_persons_predicted_correctly = {}\n",
    "\n",
    "for frame_series, labels in valloader_series:\n",
    "    for i in range(len(labels)):\n",
    "        with torch.no_grad():\n",
    "            logps = cnn_lstm_model(frame_series)\n",
    "\n",
    "    \n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = labels.numpy()[i]\n",
    "    \n",
    "    number_of_frames_with_n_persons[pred_label] = \\\n",
    "        number_of_frames_with_n_persons.get(pred_label, 0) + 1\n",
    "    \n",
    "    if true_label == pred_label:\n",
    "        correct_count += 1\n",
    "        number_of_frames_with_n_persons_predicted_correctly[pred_label] = \\\n",
    "            number_of_frames_with_n_persons_predicted_correctly.get(pred_label, 0) + 1\n",
    "    \n",
    "    tested_frames += 1\n",
    "    \n",
    "    print(f'{correct_count}/{tested_frames}')\n",
    "    \n",
    "\n",
    "print(f\"Number of tested frames: {tested_frames}\")\n",
    "print(f\"Model Accuracy = {correct_count / tested_frames}\")\n",
    "print('Predicted:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons.items()]))\n",
    "print('Predicted correctly:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons_predicted_correctly.items()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
