{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd73e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time \n",
    "from random import randrange\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import importlib\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import config\n",
    "\n",
    "from model_training import training_data_loader\n",
    "from model_training.training_data_loader import AugmentedBatchesTrainingData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cpu_device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764053e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9270a91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e119aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8dcd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIRS_1 = [\n",
    "    '31_03_21__318__3or4_people/1/006__11_44_59',\n",
    "    '31_03_21__318__3or4_people/1/007__11_48_59',\n",
    "    '31_03_21__318__3or4_people/1/008__11_52_59',\n",
    "    '31_03_21__318__3or4_people/1/009__11_57_00',\n",
    "     ]\n",
    "\n",
    "TRAINING_DIRS_2 = [\n",
    "    '31_03_21__318__3or4_people/2/000__14_15_19',\n",
    "    '31_03_21__318__3or4_people/2/001__14_19_19',\n",
    "    '31_03_21__318__3or4_people/2/002__14_23_19',\n",
    "    '31_03_21__318__3or4_people/2/003__14_27_20',\n",
    "    '31_03_21__318__3or4_people/2/004__14_31_20',\n",
    "\n",
    "    '31_03_21__318__3or4_people/2/010__14_55_20',\n",
    "    '31_03_21__318__3or4_people/2/011__14_59_20',\n",
    "    '31_03_21__318__3or4_people/2/012__15_03_21',\n",
    "    '31_03_21__318__3or4_people/2/013__15_07_21',\n",
    "    '31_03_21__318__3or4_people/2/014__15_11_21',\n",
    "    '31_03_21__318__3or4_people/2/015__15_15_21',\n",
    "    '31_03_21__318__3or4_people/2/016__15_19_21',\n",
    "    ]\n",
    "\n",
    "VALIDATION_DIRS_1 = [\n",
    "    '31_03_21__318__3or4_people/2/005__14_35_20',\n",
    "#     '31_03_21__318__3or4_people/2/006__14_39_20',\n",
    "    '31_03_21__318__3or4_people/2/007__14_43_20',\n",
    "#     '31_03_21__318__3or4_people/2/008__14_47_20',\n",
    "    '31_03_21__318__3or4_people/2/009__14_51_20',\n",
    "    \n",
    "#     '05_05_2021__0to5_people/007__13_22_20'\n",
    "]\n",
    "\n",
    "_training_data_1 = training_data_loader.load_data_for_labeled_batches(labeled_batch_dirs=TRAINING_DIRS_1)\n",
    "_training_data_2 = training_data_loader.load_data_for_labeled_batches(labeled_batch_dirs=TRAINING_DIRS_2)\n",
    "_validation_data_1 = training_data_loader.load_data_for_labeled_batches(labeled_batch_dirs=VALIDATION_DIRS_1)\n",
    "\n",
    "augmented_data_training = training_data_loader.AugmentedBatchesTrainingData()\n",
    "augmented_data_training.add_training_batch(_training_data_1)\n",
    "augmented_data_training.add_training_batch(_training_data_2)\n",
    "\n",
    "augmented_data_validation = training_data_loader.AugmentedBatchesTrainingData()\n",
    "augmented_data_validation.add_training_batch(_validation_data_1, flip_and_rotate=False)\n",
    "\n",
    "\n",
    "\n",
    "# for centre_points in _training_data_2.centre_points:\n",
    "#     if len(centre_points) > 4:\n",
    "#         print(\"Too many people on one frame in annotations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815043b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa1b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66b825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data_training.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d056f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data_validation.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f1dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d0bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce89916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_airbrush_circle(img, centre, radius):\n",
    "    for x in range(max(0, round(centre[0]-radius)), min(img.shape[0], round(centre[0]+radius+1))):\n",
    "        for y in range(max(0, round(centre[1]-radius)), min(img.shape[1], round(centre[1]+radius+1))):\n",
    "            point = (x, y)\n",
    "            distance_to_centre = cv2.norm((centre[0] - x, centre[1] - y))\n",
    "            if distance_to_centre > radius:\n",
    "                continue\n",
    "            img[point] += 1 - distance_to_centre / radius\n",
    "            \n",
    "\n",
    "def draw_cross(img, centre, cross_width, cross_height):\n",
    "    for x in range(max(0, round(centre[0]) - cross_width), min(img.shape[0], round(centre[0]) + cross_width + 1)):\n",
    "        for y in range(max(0, round(centre[1]) - cross_height), min(img.shape[1], round(centre[1]) + cross_height + 1)):\n",
    "            point = (x, y)\n",
    "            img[point] = 1\n",
    "    \n",
    "    for x in range(max(0, round(centre[0] - cross_height)), min(img.shape[0], round(centre[0] + cross_height + 1))):\n",
    "        for y in range(max(0, round(centre[1] - cross_width)), min(img.shape[1], round(centre[1] + cross_width + 1))):\n",
    "            point = (x, y)\n",
    "            img[point] = 1\n",
    "    \n",
    "    \n",
    "def get_img_reconstructed_from_labels(centre_points):\n",
    "    img_reconstructed = np.zeros(shape=(config.IR_CAMERA_RESOLUTION[0], \n",
    "                                 config.IR_CAMERA_RESOLUTION[1]))\n",
    "\n",
    "    for centre_point in centre_points:\n",
    "        centre_point = centre_point[::-1]  # reversed x and y in \n",
    "        draw_airbrush_circle(img=img_reconstructed, \n",
    "                   centre=[round(c) for c in centre_point], \n",
    "                   radius=6)\n",
    "    \n",
    "    #img_int = (img_reconstructed * (NUMBER_OF_OUPUT_CLASSES-1)).astype('int')\n",
    "    #return img_int\n",
    "    return img_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471f30d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp = _validation_data_1.centre_points[11]\n",
    "# img = get_img_reconstructed_from_labels(cp)\n",
    "# plt.imshow(img)\n",
    "# print(np.sum(img))\n",
    "\n",
    "sum_of_values_for_one_person = 35  # total sum of pixels for one person on the reconstructed image, of course it changes with circle radius, etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65ed93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b09c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetOutputMode:\n",
    "    PEOPLE_COUNT = 'PEOPLE_COUNT'\n",
    "    RECONSTRUCTED_IMG = 'RECONSTRUCTED_IMG'\n",
    "    BOTH_PEOPLE_COUNT_AND_RECONSTRUCTED_IMG = 'BOTH_PEOPLE_COUNT_AND_RECONSTRUCTED_IMG'\n",
    "    \n",
    "\n",
    "class IrPersonsUnetTrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, augmented_data: AugmentedBatchesTrainingData, mode=DatasetOutputMode.RECONSTRUCTED_IMG):\n",
    "        self.augmented_data = AugmentedBatchesTrainingData\n",
    "        self._index_to_batch_and_subindex_map = {}\n",
    "        self._index_to_variance_map = {}\n",
    "        self._mode = mode\n",
    "        self._cache = {}\n",
    "        \n",
    "        i = 0\n",
    "        for b, batch in enumerate(augmented_data.batches):\n",
    "            frames_in_batch = np.zeros(shape=(len(batch.normalized_ir_data), *batch.normalized_ir_data[0].shape))\n",
    "            #print(frames_in_batch.shape)\n",
    "            i0 = i\n",
    "            \n",
    "            for j in range(len(batch.normalized_ir_data)):\n",
    "                self._index_to_batch_and_subindex_map[i] = (batch, j) \n",
    "                frames_in_batch[j] = batch.normalized_ir_data[j]\n",
    "                i += 1\n",
    "            \n",
    "            i = i0\n",
    "            for j in range(len(batch.normalized_ir_data)):\n",
    "                variance_frames_range = 10\n",
    "                subframes = frames_in_batch[max(0, j - variance_frames_range):j + variance_frames_range]\n",
    "                v = np.std(subframes, axis=(0)) * 20\n",
    "                v = np.clip(v, 0, 1)\n",
    "                self._index_to_variance_map[i] = v\n",
    "                i += 1\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len (self._index_to_batch_and_subindex_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            raise Exception(\"Not supported\")\n",
    "        \n",
    "        if idx not in self._cache:\n",
    "            batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "            frame = batch.normalized_ir_data[subindex]\n",
    "\n",
    "            batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "            centre_points = batch.centre_points[subindex]\n",
    "            img_reconstructed = get_img_reconstructed_from_labels(centre_points)\n",
    "\n",
    "            frame_variance = self._index_to_variance_map[idx]\n",
    "            frame_3d = np.stack((frame, frame_variance))\n",
    "            \n",
    "            number_of_people = len(batch.centre_points[subindex])\n",
    "            if self._mode == DatasetOutputMode.RECONSTRUCTED_IMG:\n",
    "                result = frame_3d, img_reconstructed\n",
    "            elif self._mode == DatasetOutputMode.PEOPLE_COUNT:\n",
    "                result = frame_3d, number_of_people\n",
    "            else:\n",
    "                result = frame_3d, (number_of_people, img_reconstructed)\n",
    "            \n",
    "            self._cache[idx] = result\n",
    "            \n",
    "        return self._cache[idx]\n",
    "\n",
    "    def get_number_of_persons_for_frame(self, idx):\n",
    "        batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "        return len(batch.centre_points[subindex])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "training_dataset = IrPersonsUnetTrainDataset(augmented_data_training)\n",
    "validation_dataset = IrPersonsUnetTrainDataset(augmented_data_validation)\n",
    "\n",
    "\n",
    "# it makes no sense to split all data, as most of the frames are almost identical\n",
    "# training_dataset, validation_dataset = torch.utils.data.random_split(all_data_dataset, [training_data_len, validation_data_len])\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(training_dataset, batch_size=16, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(validation_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "print(len(trainloader))\n",
    "print(len(valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f9ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loader_iterator = iter(valloader)\n",
    "\n",
    "# for i in range(450):\n",
    "#     xb, yb = next(loader_iterator)\n",
    "\n",
    "#     if i < 400:\n",
    "#         continue\n",
    "    \n",
    "#     xb.shape, yb.shape\n",
    "\n",
    "#     print('#'*30)\n",
    "# #     plt.imshow(yb[0].numpy().squeeze())\n",
    "# #     plt.show()\n",
    "# #     plt.imshow(xb[0][0].numpy().squeeze())\n",
    "# #     plt.show()\n",
    "#     plt.imshow(xb[0][1].numpy().squeeze())\n",
    "#     plt.show()\n",
    "\n",
    "#     print(f'number of persosn based on sum: {np.sum(yb[0].numpy()) / sum_of_values_for_one_person:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec227b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = xb[0][1].numpy()\n",
    "# # m = np.clip(m, 0, 1)\n",
    "# np.max(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303e475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b164245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19582ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139112e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 336\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786773ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = self.contract_block(in_channels, 32, 5, 2)\n",
    "        self.conv2 = self.contract_block(32, 64, 3, 1)\n",
    "        #self.conv3 = self.contract_block(64, 128, 3, 1)\n",
    "\n",
    "        #self.upconv3 = self.expand_block(128, 64, 3, 1)\n",
    "        self.upconv2 = self.expand_block(64, 32, 3, 1)\n",
    "        self.upconv1 = self.expand_block(32*2, out_channels, 3, 1)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "\n",
    "        # downsampling part\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        #conv3 = self.conv3(conv2)\n",
    "\n",
    "        #upconv3 = self.upconv3(conv3)\n",
    "\n",
    "        #upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n",
    "        upconv2 = self.upconv2(conv2)\n",
    "        \n",
    "        upconv1 = self.upconv1(torch.cat([upconv2, conv1], 1))\n",
    "\n",
    "        upconv1_single = upconv1[:, 0, : ,:]\n",
    "        \n",
    "#         print(f'upconv1.shape = {upconv1.shape}')\n",
    "#         print(f'upconv1_single.shape = {upconv1_single.shape}')\n",
    "\n",
    "        #return upconv1\n",
    "        return upconv1_single\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        contract = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "                                 )\n",
    "\n",
    "        return contract\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1) \n",
    "                            )\n",
    "        return expand\n",
    "    \n",
    "    \n",
    "unet = UNET(2, 1).double()\n",
    "unet = unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=unet\n",
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "# params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=unet\n",
    "# sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eaaac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd24f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa4907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84933f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(loader, model):\n",
    "    correct_count = 0\n",
    "    tested_frames = 0\n",
    "    number_of_frames_with_n_persons = {}\n",
    "    number_of_frames_with_n_persons_predicted_correctly = {}\n",
    "\n",
    "    persons_error_sum = 0\n",
    "\n",
    "    for frame, labels in loader:\n",
    "        for i in range(len(labels)):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(frame.to(device)).to(cpu_device)\n",
    "            predicted_img = outputs[i].numpy()\n",
    "\n",
    "            pred_people = np.sum(predicted_img) / sum_of_values_for_one_person\n",
    "            pred_label = round(pred_people)\n",
    "\n",
    "            true_label = round(np.sum(labels.numpy()[i]) / sum_of_values_for_one_person)  # not true entirely, but good enough for testing. One would need to obtain real number of people on image\n",
    "\n",
    "            persons_error_sum += abs(pred_people - true_label)\n",
    "\n",
    "            number_of_frames_with_n_persons[pred_label] = \\\n",
    "                number_of_frames_with_n_persons.get(pred_label, 0) + 1\n",
    "\n",
    "            if true_label == pred_label:\n",
    "                correct_count += 1\n",
    "                number_of_frames_with_n_persons_predicted_correctly[pred_label] = \\\n",
    "                    number_of_frames_with_n_persons_predicted_correctly.get(pred_label, 0) + 1\n",
    "\n",
    "            tested_frames += 1\n",
    "\n",
    "\n",
    "    average_prediction_error = persons_error_sum / tested_frames\n",
    "    model_accuracy = correct_count / tested_frames\n",
    "    \n",
    "    print(f\"Number of tested frames: {tested_frames}\")\n",
    "    print(f\"Model Accuracy = {model_accuracy}\")\n",
    "    print('Predicted:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons.items()]))\n",
    "    print('Predicted correctly:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons_predicted_correctly.items()]))\n",
    "    print(f'average_prediction_error: {average_prediction_error}')\n",
    "    \n",
    "    return model_accuracy, average_prediction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04ea7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705738a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_dl, valid_dl, loss_fn, optimizer, epochs=1):\n",
    "    start = time.time()\n",
    "    train_loss, valid_loss, valid_error = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set trainind mode = true\n",
    "                dataloader = train_dl\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = valid_dl\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            step = 0\n",
    "            for x, y in dataloader:\n",
    "                if randrange(100) != 1:  # do not train on every frame in each epoch\n",
    "                    if step != 0:\n",
    "                        continue\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = loss_fn(outputs, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(x)\n",
    "                        loss = loss_fn(outputs, y)\n",
    "                    if step == 1:\n",
    "                        plt.imshow(outputs[0].to(cpu_device))\n",
    "                        plt.show()\n",
    "                        \n",
    "                        if (epoch+1) % 10 == 0:\n",
    "                            # full validation of the model\n",
    "                            accuracy, prediction_error = validate_model(loader=valid_dl, model=model)\n",
    "                            valid_error.append(prediction_error)\n",
    "                                \n",
    "\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                          \n",
    "            epoch_loss = running_loss / step\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}. lr={lr}')\n",
    "            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n",
    "            \n",
    "        if epoch > 10 and valid_error and valid_error[-1] < 0.12:  # TODO - different value for different validation data\n",
    "            print('Training finished, results good enough...')\n",
    "            break\n",
    "            \n",
    "            \n",
    "    time_elapsed = time.time() - start\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')    \n",
    "    return train_loss, valid_loss, valid_error    \n",
    "\n",
    "  \n",
    "# sometimes one need to reinitialize the weights, after a few epcochs (like 10) the result should be already good)\n",
    "# unet = UNET(2, 1).double()\n",
    "# unet = unet.to(device)\n",
    "\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "# loss_fn = nn.MSELoss()  # sometimes learn strangely\n",
    "\n",
    "opt = torch.optim.Adam(unet.parameters(), lr=0.002)\n",
    "train_loss, valid_loss, valid_error,  = train(model=unet, \n",
    "                               train_dl=trainloader, \n",
    "                               valid_dl=valloader, \n",
    "                               loss_fn=loss_fn, \n",
    "                               optimizer=opt, \n",
    "                               epochs=200)\n",
    "\n",
    "# if train_loss[-1] > 0.04:\n",
    "#     raise Exception(\"Training error, reinitialize weights. Network sometimes doesn't learn as it should\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.legend(['train', 'valid'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf62d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(valid_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec4f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187311e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for _ in range(22):\n",
    "#     xb, yb = next(iter(valloader))\n",
    "#     #xb, yb = next(iter(train_d1))\n",
    "    \n",
    "#     plt.imshow(xb[0].numpy().squeeze())\n",
    "#     plt.show()\n",
    "#     plt.imshow(yb[0].numpy().squeeze())\n",
    "#     plt.show()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = unet(xb.to(device)).to(cpu_device)\n",
    "    \n",
    "#     predicted_img = outputs[0].numpy()\n",
    "#     plt.imshow(predicted_img)\n",
    "#     plt.show()\n",
    "#     print(f'number of persosn based on sum: {np.sum(predicted_img) / sum_of_values_for_one_person:.3f}')\n",
    "#     print(\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30bb17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model(loader=valloader, model=unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be3f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af76f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018bc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697aded9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f99a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199fb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3fab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d623ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DIRS_2 = [\n",
    "    '05_05_2021__0to5_people/012__13_42_20',\n",
    "    '05_05_2021__0to5_people/013__13_46_21',\n",
    "    '05_05_2021__0to5_people/014__13_50_21',\n",
    "    '05_05_2021__0to5_people/015__13_54_21',\n",
    "    \n",
    "    '05_05_2021__0to5_people/007__13_22_20',\n",
    "    \n",
    "    '31_03_21__318__3or4_people/2/005__14_35_20',\n",
    "    '31_03_21__318__3or4_people/2/006__14_39_20',\n",
    "    '31_03_21__318__3or4_people/2/007__14_43_20',\n",
    "    '31_03_21__318__3or4_people/2/008__14_47_20',\n",
    "    '31_03_21__318__3or4_people/2/009__14_51_20',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_with_people_count = training_data_loader.load_data_for_labeled_batches(labeled_batch_dirs=VALIDATION_DIRS_2)\n",
    "\n",
    "augmented_data_with_people_count = training_data_loader.AugmentedBatchesTrainingData()\n",
    "augmented_data_with_people_count.add_training_batch(data_with_people_count, flip_and_rotate=False)\n",
    "\n",
    "\n",
    "dataset_with_real_people_count = IrPersonsUnetTrainDataset(augmented_data_with_people_count, mode=DatasetOutputMode.BOTH_PEOPLE_COUNT_AND_RECONSTRUCTED_IMG)\n",
    "loader_with_real_people_count = torch.utils.data.DataLoader(dataset_with_real_people_count, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede86b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for frame, labels in loader_with_real_people_count:\n",
    "#     print(labels[1].shape)\n",
    "#     print(labels[0].numpy()[0])\n",
    "#     print(labels[1][0].numpy())\n",
    "#     break\n",
    "# len(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9531f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_model_with_real_number_of_persons(loader, model, data_plotting_interval=1000):\n",
    "    correct_count = 0\n",
    "    tested_frames = 0\n",
    "    number_of_frames_with_n_persons = {}\n",
    "    number_of_frames_with_n_persons_predicted_correctly = {}\n",
    "\n",
    "    persons_error_sum = 0\n",
    "    \n",
    "    vec_real_number_of_persons = []\n",
    "    vec_reconstructed_number_of_persons = []  # calculated from reconstructed image for training\n",
    "    vec_predicted_number_of_persons = []\n",
    "\n",
    "    for frame, people_count_and_reconstructed_image in loader:\n",
    "        for i in range(len(frame)):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(frame.to(device)).to(cpu_device)\n",
    "#             print(outputs.shape)\n",
    "            predicted_img = outputs[i].numpy()\n",
    "\n",
    "            pred_people = np.sum(predicted_img) / sum_of_values_for_one_person\n",
    "            pred_label = round(pred_people)\n",
    "            \n",
    "\n",
    "            true_people_count = people_count_and_reconstructed_image[0].numpy()[0]\n",
    "            reconstructed_image = people_count_and_reconstructed_image[1][0].numpy()  # training data for unet\n",
    "            vec_reconstructed_number_of_persons.append(np.sum(reconstructed_image) / sum_of_values_for_one_person)\n",
    "            \n",
    "            persons_error_sum += abs(pred_people - true_people_count)\n",
    "\n",
    "            number_of_frames_with_n_persons[pred_label] = \\\n",
    "                number_of_frames_with_n_persons.get(pred_label, 0) + 1\n",
    "\n",
    "            if true_people_count == pred_label:\n",
    "                correct_count += 1\n",
    "                number_of_frames_with_n_persons_predicted_correctly[pred_label] = \\\n",
    "                    number_of_frames_with_n_persons_predicted_correctly.get(pred_label, 0) + 1\n",
    "\n",
    "            tested_frames += 1\n",
    "            \n",
    "            vec_real_number_of_persons.append(true_people_count)\n",
    "            vec_predicted_number_of_persons.append(pred_people)\n",
    "            \n",
    "            if tested_frames % data_plotting_interval == 0:\n",
    "                plt.imshow(frame[i, 0, :, :])\n",
    "                plt.show()\n",
    "                print(f'true_people_count={true_people_count}, pred_people={pred_people}')\n",
    "                plt.imshow(predicted_img)\n",
    "                plt.show()\n",
    "                print('#'*30)\n",
    "\n",
    "\n",
    "    average_prediction_error = persons_error_sum / tested_frames\n",
    "    model_accuracy = correct_count / tested_frames\n",
    "    \n",
    "    print(f\"Number of tested frames: {tested_frames}\")\n",
    "    print(f\"Model Accuracy = {model_accuracy}\")\n",
    "    print('Predicted:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons.items()]))\n",
    "    print('Predicted correctly:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons_predicted_correctly.items()]))\n",
    "    print(f'average_prediction_error: {average_prediction_error}')\n",
    "    \n",
    "    return model_accuracy, average_prediction_error, vec_real_number_of_persons, \\\n",
    "            vec_predicted_number_of_persons, vec_reconstructed_number_of_persons\n",
    "\n",
    "\n",
    "_, _, real_vec, predicted_vec, vec_reconstructed_number_of_persons = \\\n",
    "        validate_model_with_real_number_of_persons(loader=loader_with_real_people_count, model=unet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib\n",
    "fig=plt.figure(figsize=(8,6), dpi= 100, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.grid()\n",
    "plt.stairs(real_vec, linewidth=2.5)\n",
    "plt.stairs(predicted_vec, linewidth=1)\n",
    "\n",
    "plt.title('number of people')\n",
    "plt.legend(['real', 'predicted'])\n",
    "plt.xlabel('time [0.5*s]')\n",
    "plt.ylabel('people count')\n",
    "# plt.stairs(vec_reconstructed_number_of_persons)\n",
    "\n",
    "# plt.show()\n",
    "# plt.plot(np.array(vec_reconstructed_number_of_persons) - np.array(predicted_vec))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f76bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2e80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87432e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
