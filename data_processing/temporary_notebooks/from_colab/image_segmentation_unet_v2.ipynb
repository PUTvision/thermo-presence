{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaTSxqAkIp3l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time \n",
    "import random\n",
    "from random import randrange\n",
    "import json\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import importlib\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nxn6v3wqYI4I"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cpu_device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 43\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gBEE8_EYJZW"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "#drive.mount('/content/drive/')\n",
    "\n",
    "# with open('/content/drive/My Drive/wspoldzielone/ir/05_05_2021__0to5_people--013__13_46_21.csv', 'r') as f:\n",
    "#   print(f.read())\n",
    "\n",
    "\n",
    "ROOT_DATA_DIR_PATH = '/media/data/temporary/thermo-presence/'\n",
    "OUTPUT_LABELS_DIR = '/home/przemek/Projects/thermo-presence/thermo-presence/data_processing/data_labeling/labeling_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cp2paP8NYJkS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfiVqcRvJB-E"
   },
   "outputs": [],
   "source": [
    "IR_CAMERA_RESOLUTION_X = 32\n",
    "IR_CAMERA_RESOLUTION_Y = 24\n",
    "\n",
    "IR_CAMERA_RESOLUTION = (IR_CAMERA_RESOLUTION_Y, IR_CAMERA_RESOLUTION_X)\n",
    "\n",
    "IR_CAMERA_RESOLUTION_XY = (IR_CAMERA_RESOLUTION_X, IR_CAMERA_RESOLUTION_Y)  # for opencv functions\n",
    "\n",
    "# for frames normalization\n",
    "TEMPERATURE_NORMALIZATION__MIN = 20\n",
    "TEMPERATURE_NORMALIZATION__MAX = 35\n",
    "\n",
    "IR_FRAME_RESIZE_MULTIPLIER = 1\n",
    "\n",
    "MIN_TEMPERATURE_ON_PLOT = 20  # None for auto range\n",
    "MAX_TEMPERATURE_ON_PLOT = 30  # None for auto range\n",
    "IR_FRAME_INTERPOLATION_METHOD = cv2.INTER_CUBIC\n",
    "RGB_FRAME_RESIZE_MULTIPLIER = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYQfi2DiJCA2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3L9TjWVIJCDP",
    "outputId": "cb1d11e3-5f5d-4c19-b144-e7b166ae4217"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpPEWh6bJcet"
   },
   "outputs": [],
   "source": [
    "# pretrained_encoder_file_path = '/content/drive/My Drive/wspoldzielone/ir/cifar10_encder_weights_v1.p'\n",
    "pretrained_encoder_file_path = '/home/przemek/Downloads/cifar10_encoder_weights_v1.p'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwOyXW7sJcpt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "375jtwrkJcs1"
   },
   "outputs": [],
   "source": [
    "class IrDataCsvReader:\n",
    "    def __init__(self, file_path):\n",
    "        self._file_path = file_path\n",
    "        self._frames, self._raw_frame_data = self.get_frames_from_file(file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_frames_from_file(file_path):\n",
    "        frames = []\n",
    "        raw_frame_data = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            raw_lines = file.readlines()\n",
    "        lines = [x.strip() for x in raw_lines]\n",
    "        frame_lines = lines[1:]\n",
    "        for frame_line in frame_lines:\n",
    "            raw_frame_data.append(frame_line)\n",
    "            line_parts = frame_line.split(',')\n",
    "            frame_data_str = line_parts[2:]\n",
    "            frame_data_1d = [float(x) for x in frame_data_str]\n",
    "            frame_2d = np.reshape(frame_data_1d, IR_CAMERA_RESOLUTION)\n",
    "            frames.append(frame_2d)\n",
    "        return frames, raw_frame_data\n",
    "\n",
    "    def get_number_of_frames(self):\n",
    "        return len(self._frames)\n",
    "\n",
    "    def get_frame(self, n):\n",
    "        return self._frames[n]\n",
    "\n",
    "    def get_raw_frame_data(self, n) -> str:\n",
    "        return self._raw_frame_data[n]\n",
    "\n",
    "\n",
    "\n",
    "class BatchTrainingData:\n",
    "    \"\"\"\n",
    "    Stores training data for one batch\n",
    "    \"\"\"\n",
    "    def __init__(self, min_temperature=TEMPERATURE_NORMALIZATION__MIN, max_temperature=TEMPERATURE_NORMALIZATION__MAX):\n",
    "        self.centre_points = []  # type: List[List[tuple]]\n",
    "        self.raw_ir_data = []  # type: List[np.ndarray]\n",
    "        self.normalized_ir_data = []  # type: List[np.ndarray]  # same data as raw_ir_data, but normalized\n",
    "\n",
    "        self.min_temperature = min_temperature\n",
    "        self.max_temperature = max_temperature\n",
    "\n",
    "    def append_frame_data(self, centre_points, raw_ir_data):\n",
    "        self.centre_points.append(centre_points)\n",
    "        self.raw_ir_data.append(raw_ir_data)\n",
    "\n",
    "        ir_data_normalized = (raw_ir_data - self.min_temperature) * (1 / (self.max_temperature - self.min_temperature))\n",
    "        self.normalized_ir_data.append(ir_data_normalized)\n",
    "\n",
    "    def flip_horizontally(self):\n",
    "        for i in range(len(self.raw_ir_data)):\n",
    "            self.raw_ir_data[i] = np.flip(self.raw_ir_data[i], 1)\n",
    "            self.normalized_ir_data[i] = np.flip(self.normalized_ir_data[i], 1)\n",
    "            for j in range(len(self.centre_points[i])):\n",
    "                x_flipped = IR_CAMERA_RESOLUTION_X - self.centre_points[i][j][0]\n",
    "                self.centre_points[i][j] = (x_flipped, self.centre_points[i][j][1])\n",
    "\n",
    "    def flip_vertically(self):\n",
    "        for i in range(len(self.raw_ir_data)):\n",
    "            self.raw_ir_data[i] = np.flip(self.raw_ir_data[i], 0)\n",
    "            self.normalized_ir_data[i] = np.flip(self.normalized_ir_data[i], 0)\n",
    "            for j in range(len(self.centre_points[i])):\n",
    "                y_flipped = IR_CAMERA_RESOLUTION_Y - self.centre_points[i][j][1]\n",
    "                self.centre_points[i][j] = (self.centre_points[i][j][0], y_flipped)\n",
    "\n",
    "\n",
    "class AugmentedBatchesTrainingData:\n",
    "    \"\"\"\n",
    "    Stores training data for all batches, with data augmentation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.batches = []  # Type: List[BatchTrainingData]\n",
    "\n",
    "    def add_training_batch(self, batch: BatchTrainingData, flip_and_rotate=True):\n",
    "        self.batches.append(copy.deepcopy(batch))  # plain data\n",
    "\n",
    "        if flip_and_rotate:\n",
    "            batch.flip_horizontally()\n",
    "            self.batches.append(copy.deepcopy(batch))  # flipped horizontally\n",
    "\n",
    "            batch.flip_vertically()\n",
    "            self.batches.append(copy.deepcopy(batch))  # rotated 180 degrees\n",
    "\n",
    "            batch.flip_horizontally()\n",
    "            self.batches.append(copy.deepcopy(batch))  # rotated vertically\n",
    "\n",
    "    def print_stats(self):\n",
    "        total_number_of_frames = 0\n",
    "        number_of_frames_with_n_persons = {}\n",
    "        for batch in self.batches:\n",
    "            total_number_of_frames += len(batch.centre_points)\n",
    "            for centre_points in batch.centre_points:\n",
    "                number_of_persons = len(centre_points)\n",
    "                number_of_frames_with_n_persons[number_of_persons] = \\\n",
    "                    number_of_frames_with_n_persons.get(number_of_persons, 0) + 1\n",
    "        frames_persons_details_msg = '\\n'.join([f'   {count} frames with {no} persons'\n",
    "                                               for no, count in number_of_frames_with_n_persons.items()])\n",
    "        msg = f\"AugmentedBatchesTrainingData with {len(self.batches)} BatchTrainingData batches.\\n\" \\\n",
    "              f\"Total number of frames after augmentation: {total_number_of_frames}, with:\\n\" \\\n",
    "              f\"{frames_persons_details_msg}\"\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def load_data_for_labeled_batches(labeled_batch_dirs) -> BatchTrainingData:\n",
    "\n",
    "    training_data = BatchTrainingData()\n",
    "    for batch_subdir in labeled_batch_dirs:\n",
    "        data_batch_dir_path = os.path.join(ROOT_DATA_DIR_PATH, batch_subdir)\n",
    "        raw_ir_data_csv_file_path = os.path.join(data_batch_dir_path, 'ir.csv')\n",
    "        output_file_with_labels_name = batch_subdir.replace('/', '--') + '.csv'\n",
    "        annotation_data_file_path = os.path.join(OUTPUT_LABELS_DIR, output_file_with_labels_name)\n",
    "\n",
    "        raw_ir_data_csv_reader = IrDataCsvReader(file_path=raw_ir_data_csv_file_path)\n",
    "        annotations_collector = AnnotationCollector.load_from_file(\n",
    "\n",
    "            file_path=annotation_data_file_path, do_not_scale_and_reverse=True)\n",
    "\n",
    "        for frame_index in range(raw_ir_data_csv_reader.get_number_of_frames()):\n",
    "            raw_frame_data = raw_ir_data_csv_reader.get_frame(frame_index)\n",
    "            frame_annotations = annotations_collector.get_annotation(frame_index)\n",
    "            if not frame_annotations.accepted:\n",
    "                print(f\"Frame index {frame_index} from batch '{batch_subdir}' not annotated!\")\n",
    "                continue\n",
    "            training_data.append_frame_data(\n",
    "                centre_points=frame_annotations.centre_points,\n",
    "                raw_ir_data=raw_frame_data)\n",
    "\n",
    "    return training_data\n",
    "\n",
    "\n",
    "\n",
    "class AnnotationCollector:\n",
    "    ANNOTATIONS_BETWEEN_AUTOSAVE = 10\n",
    "\n",
    "    def __init__(self, output_file_path, data_batch_dir_path):\n",
    "        self._output_file_path = output_file_path\n",
    "        self._annotations = {}  # ir_frame_index: FrameAnnotation\n",
    "        self._data_batch_dir_path = data_batch_dir_path\n",
    "        self._annotations_to_autosave = 1\n",
    "\n",
    "    def get_annotation(self, ir_frame_index):\n",
    "        return self._annotations.get(ir_frame_index, FrameAnnotation())\n",
    "\n",
    "    def set_annotation(self, ir_frame_index, annotation):\n",
    "        self._annotations[ir_frame_index] = annotation\n",
    "        if annotation.accepted or annotation.discarded:\n",
    "            self._annotations_to_autosave -= 1\n",
    "            if self._annotations_to_autosave == 0:\n",
    "                self._annotations_to_autosave = self.ANNOTATIONS_BETWEEN_AUTOSAVE\n",
    "                self.save()\n",
    "\n",
    "    def save(self):\n",
    "        data_dict = {\n",
    "            'output_file_path': self._output_file_path,\n",
    "            'data_batch_dir_path': self._data_batch_dir_path,\n",
    "            'annotations': {index: annotation.as_dict()\n",
    "                            for index, annotation in self._annotations.items()}\n",
    "        }\n",
    "        with open(self._output_file_path, 'w') as file:\n",
    "            file.write(json.dumps(data_dict, indent=2))\n",
    "            file.flush()\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_file(cls, file_path, do_not_scale_and_reverse=False):\n",
    "        item = cls(output_file_path=file_path, data_batch_dir_path=None)\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.read()\n",
    "        data_dict = json.loads(data)\n",
    "        item._data_batch_dir_path = data_dict['data_batch_dir_path']\n",
    "        item._annotations = {int(index): FrameAnnotation.from_dict(annotation_dict, do_not_scale_and_reverse) for index, annotation_dict\n",
    "                             in data_dict['annotations'].items()}\n",
    "        return item\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "\n",
    "def x_on_interpolated_image_to_raw_x(x):\n",
    "    x_raw_flipped = x / IR_FRAME_RESIZE_MULTIPLIER\n",
    "    x_raw = IR_CAMERA_RESOLUTION_X - x_raw_flipped\n",
    "    return x_raw\n",
    "\n",
    "\n",
    "def y_on_interpolated_image_to_raw_y(y):\n",
    "    return y / IR_FRAME_RESIZE_MULTIPLIER\n",
    "\n",
    "\n",
    "def xy_on_interpolated_image_to_raw_xy(xy: tuple) -> tuple:\n",
    "    return (x_on_interpolated_image_to_raw_x(xy[0]),\n",
    "            y_on_interpolated_image_to_raw_y(xy[1]))\n",
    "\n",
    "class FrameAnnotation:\n",
    "    def __init__(self):\n",
    "        self.accepted = False  # whether frame was marked as annotated successfully\n",
    "        self.discarded = False  # whether frame was marked as discarded (ignored)\n",
    "        self.centre_points = []  # type: List[tuple]  # x, y\n",
    "        self.rectangles = []  # type: List[Tuple[tuple, tuple]]  # (x_left, y_top), (x_right, y_bottom)\n",
    "\n",
    "        self.raw_frame_data = None  # Not an annotation, but write it to the result file, just in case\n",
    "\n",
    "    def as_dict(self):\n",
    "        data_dict = copy.copy(self.__dict__)\n",
    "        data_dict['centre_points'] = []\n",
    "        data_dict['rectangles'] = []\n",
    "\n",
    "        for i, point in enumerate(self.centre_points):\n",
    "            data_dict['centre_points'].append(xy_on_interpolated_image_to_raw_xy(point))\n",
    "        for i, rectangle in enumerate(self.rectangles):\n",
    "            data_dict['rectangles'].append((xy_on_interpolated_image_to_raw_xy(rectangle[0]),\n",
    "                                            xy_on_interpolated_image_to_raw_xy(rectangle[1])))\n",
    "        return data_dict\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data_dict, do_not_scale_and_reverse=False):\n",
    "        item = cls()\n",
    "        item.__dict__.update(data_dict)\n",
    "        for i, point in enumerate(item.centre_points):\n",
    "            item.centre_points[i] = xy_on_raw_image_to_xy_on_interpolated_image(point, do_not_scale_and_reverse)\n",
    "        for i, rectangle in enumerate(item.rectangles):\n",
    "            item.rectangles[i] = (xy_on_raw_image_to_xy_on_interpolated_image(rectangle[0], do_not_scale_and_reverse),\n",
    "                                  xy_on_raw_image_to_xy_on_interpolated_image(rectangle[1], do_not_scale_and_reverse))\n",
    "        return item\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def x_on_interpolated_image_to_raw_x(x):\n",
    "    x_raw_flipped = x / IR_FRAME_RESIZE_MULTIPLIER\n",
    "    x_raw = IR_CAMERA_RESOLUTION_X - x_raw_flipped\n",
    "    return x_raw\n",
    "\n",
    "\n",
    "def y_on_interpolated_image_to_raw_y(y):\n",
    "    return y / IR_FRAME_RESIZE_MULTIPLIER\n",
    "\n",
    "\n",
    "def xy_on_interpolated_image_to_raw_xy(xy: tuple) -> tuple:\n",
    "    return (x_on_interpolated_image_to_raw_x(xy[0]),\n",
    "            y_on_interpolated_image_to_raw_y(xy[1]))\n",
    "\n",
    "\n",
    "def x_on_raw_image_to_x_on_interpolated_image(x):\n",
    "    x_flipped = IR_CAMERA_RESOLUTION_X - x\n",
    "    return round(x_flipped * IR_FRAME_RESIZE_MULTIPLIER)\n",
    "\n",
    "\n",
    "def y_on_raw_image_to_y_on_interpolated_image(y):\n",
    "    return round(y * IR_FRAME_RESIZE_MULTIPLIER)\n",
    "\n",
    "\n",
    "def xy_on_raw_image_to_xy_on_interpolated_image(xy: tuple, do_not_scale_and_reverse=False) -> tuple:\n",
    "    if do_not_scale_and_reverse:\n",
    "        return xy\n",
    "\n",
    "    return (x_on_raw_image_to_x_on_interpolated_image(xy[0]),\n",
    "            y_on_raw_image_to_y_on_interpolated_image(xy[1]))\n",
    "\n",
    "\n",
    "def get_extrapolated_ir_frame_heatmap_flipped(\n",
    "        frame_2d, multiplier, interpolation, min_temp, max_temp, colormap):\n",
    "    new_size = (frame_2d.shape[1] * multiplier, frame_2d.shape[0] * multiplier)\n",
    "    frame_resized_not_clipped = cv2.resize(\n",
    "        src=frame_2d, dsize=new_size, interpolation=interpolation)\n",
    "\n",
    "    if min_temp is None:\n",
    "        min_temp = min(frame_resized_not_clipped.reshape(-1))\n",
    "    if max_temp is None:\n",
    "        max_temp = max(frame_resized_not_clipped.reshape(-1))\n",
    "\n",
    "    frame_resized = np.clip(frame_resized_not_clipped, min_temp, max_temp)\n",
    "    frame_resized_normalized = (frame_resized - min_temp) * (255 / (max_temp - min_temp))\n",
    "    frame_resized_normalized_u8 = frame_resized_normalized.astype(np.uint8)\n",
    "    heatmap_u8 = (colormap(frame_resized_normalized_u8) * 2 ** 8).astype(np.uint8)[:, :, :3]\n",
    "    heatmap_u8_bgr = cv2.cvtColor(heatmap_u8, cv2.COLOR_RGB2BGR)\n",
    "    heatmap_u8_bgr_flipped = cv2.flip(heatmap_u8_bgr, 1)  # horizontal flip\n",
    "    return heatmap_u8_bgr_flipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAhpUaMzJcwT",
    "outputId": "c53c2929-be54-4545-8c53-a2e0c6354fb2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AIpGLgeyJCGB",
    "outputId": "bfddb680-71d6-4908-9832-19295029fdfc"
   },
   "outputs": [],
   "source": [
    "TRAINING_DIRS_1 = [\n",
    "    '31_03_21__318__3or4_people/1/006__11_44_59',\n",
    "    '31_03_21__318__3or4_people/1/007__11_48_59',\n",
    "    '31_03_21__318__3or4_people/1/008__11_52_59',\n",
    "    '31_03_21__318__3or4_people/1/009__11_57_00',\n",
    "     \n",
    "    \n",
    "    '31_03_21__318__3or4_people/2/000__14_15_19',\n",
    "    '31_03_21__318__3or4_people/2/001__14_19_19',\n",
    "    '31_03_21__318__3or4_people/2/002__14_23_19',\n",
    "    '31_03_21__318__3or4_people/2/003__14_27_20',\n",
    "    '31_03_21__318__3or4_people/2/004__14_31_20',\n",
    "    \n",
    "    '31_03_21__318__3or4_people/2/012__15_03_21',\n",
    "    '31_03_21__318__3or4_people/2/013__15_07_21',\n",
    "    '31_03_21__318__3or4_people/2/014__15_11_21',\n",
    "    '31_03_21__318__3or4_people/2/015__15_15_21',\n",
    "    '31_03_21__318__3or4_people/2/016__15_19_21',\n",
    "    \n",
    "    \n",
    "    '05_05_2021__0to5_people/011__13_38_20',\n",
    "    '05_05_2021__0to5_people/012__13_42_20',\n",
    "    '05_05_2021__0to5_people/013__13_46_21',\n",
    "    \n",
    "    '05_05_2021__0to5_people/007__13_22_20',\n",
    "    '05_05_2021__0to5_people/008__13_26_20',\n",
    "    ]\n",
    "\n",
    "\n",
    "VALIDATION_DIRS_1 = [\n",
    "    '05_05_2021__0to5_people/004__13_10_20',\n",
    "    \n",
    "    '05_05_2021__0to5_people/014__13_50_21',\n",
    "    '05_05_2021__0to5_people/015__13_54_21',\n",
    "    \n",
    "    '31_03_21__318__3or4_people/2/005__14_35_20',\n",
    "    '31_03_21__318__3or4_people/2/006__14_39_20',\n",
    "    '31_03_21__318__3or4_people/2/007__14_43_20',\n",
    "    '31_03_21__318__3or4_people/2/008__14_47_20',\n",
    "    '31_03_21__318__3or4_people/2/009__14_51_20',\n",
    "    '31_03_21__318__3or4_people/2/010__14_55_20',\n",
    "    '31_03_21__318__3or4_people/2/011__14_59_20',\n",
    "    \n",
    "]\n",
    "\n",
    "_training_data_1 = load_data_for_labeled_batches(labeled_batch_dirs=TRAINING_DIRS_1)\n",
    "_validation_data_1 = load_data_for_labeled_batches(labeled_batch_dirs=VALIDATION_DIRS_1)\n",
    "\n",
    "augmented_data_training = AugmentedBatchesTrainingData()\n",
    "augmented_data_training.add_training_batch(_training_data_1)\n",
    "\n",
    "augmented_data_validation = AugmentedBatchesTrainingData()\n",
    "augmented_data_validation.add_training_batch(_validation_data_1, flip_and_rotate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZaHnpXGJCId"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlG92cIEJCLx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzWSD-zWOWn_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "ggaNQeFDOWry",
    "outputId": "314e8429-0bd3-4ec9-a2ce-a9c214253210"
   },
   "outputs": [],
   "source": [
    "augmented_data_training.print_stats()\n",
    "\n",
    "def draw_airbrush_circle(img, centre, radius):\n",
    "    for x in range(max(0, round(centre[0]-radius)), min(img.shape[0], round(centre[0]+radius+1))):\n",
    "        for y in range(max(0, round(centre[1]-radius)), min(img.shape[1], round(centre[1]+radius+1))):\n",
    "            point = (x, y)\n",
    "            distance_to_centre = cv2.norm((centre[0] - x, centre[1] - y))\n",
    "            if distance_to_centre > radius:\n",
    "                continue\n",
    "            img[point] += 1 - distance_to_centre / radius\n",
    "            \n",
    "\n",
    "def draw_cross(img, centre, cross_width, cross_height):\n",
    "    for x in range(max(0, round(centre[0]) - cross_width), min(img.shape[0], round(centre[0]) + cross_width + 1)):\n",
    "        for y in range(max(0, round(centre[1]) - cross_height), min(img.shape[1], round(centre[1]) + cross_height + 1)):\n",
    "            point = (x, y)\n",
    "            img[point] = 1\n",
    "    \n",
    "    for x in range(max(0, round(centre[0] - cross_height)), min(img.shape[0], round(centre[0] + cross_height + 1))):\n",
    "        for y in range(max(0, round(centre[1] - cross_width)), min(img.shape[1], round(centre[1] + cross_width + 1))):\n",
    "            point = (x, y)\n",
    "            img[point] = 1\n",
    "            \n",
    "\n",
    "def gauss_1d(x, sig):\n",
    "    return np.exp(-np.power(x, 2.) / (2 * np.power(sig, 2.)))\n",
    "    \n",
    "    \n",
    "def draw_gauss(img, sig, centre):\n",
    "    radius = 3 * sig\n",
    "    for x in range(max(0, round(centre[0]-radius)), min(img.shape[0], round(centre[0]+radius+1))):\n",
    "        for y in range(max(0, round(centre[1]-radius)), min(img.shape[1], round(centre[1]+radius+1))):\n",
    "            point = (x, y)\n",
    "            distance_to_centre = cv2.norm((centre[0] - x, centre[1] - y))\n",
    "            if distance_to_centre > radius:\n",
    "                continue\n",
    "            gauss_value = gauss_1d(distance_to_centre, sig)\n",
    "            img[point] += gauss_value\n",
    "    \n",
    "    \n",
    "def get_img_reconstructed_from_labels(centre_points):\n",
    "    img_reconstructed = np.zeros(shape=(IR_CAMERA_RESOLUTION[0], \n",
    "                                        IR_CAMERA_RESOLUTION[1]))\n",
    "\n",
    "    for centre_point in centre_points:\n",
    "        centre_point = centre_point[::-1]  # reversed x and y in \n",
    "        draw_gauss(img=img_reconstructed, \n",
    "                   centre=[c for c in centre_point], \n",
    "                   sig=3)\n",
    "    \n",
    "    #img_int = (img_reconstructed * (NUMBER_OF_OUPUT_CLASSES-1)).astype('int')\n",
    "    #return img_int\n",
    "    return img_reconstructed\n",
    "\n",
    "\n",
    "\n",
    "cp = _validation_data_1.centre_points[777]\n",
    "img = get_img_reconstructed_from_labels(cp)\n",
    "plt.imshow(img)\n",
    "print(np.sum(img)/1)\n",
    "\n",
    "# for sig == 3, it is between 45 - 55, depending whether people are on the edge \n",
    "sum_of_values_for_one_person = 51.35  # total sum of pixels for one person on the reconstructed image, of course it changes with circle radius, etc. Calculated as average from all training data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IrPersonsUnetTrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, augmented_data: AugmentedBatchesTrainingData, transform=None):\n",
    "        self.augmented_data = AugmentedBatchesTrainingData\n",
    "        self.transform = transform\n",
    "        self._index_to_batch_and_subindex_map = {}\n",
    "        \n",
    "        self._cache = {}\n",
    "        \n",
    "        i = 0\n",
    "        for batch in augmented_data.batches:\n",
    "            for j in range(len(batch.raw_ir_data)):\n",
    "                self._index_to_batch_and_subindex_map[i] = (batch, j) \n",
    "                i += 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len (self._index_to_batch_and_subindex_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            raise Exception(\"Not supported\")\n",
    "        \n",
    "        if idx not in self._cache:\n",
    "            batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "            frame = batch.normalized_ir_data[subindex][np.newaxis, :, :]\n",
    "\n",
    "            batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "            centre_points = batch.centre_points[subindex]\n",
    "            img_reconstructed = get_img_reconstructed_from_labels(centre_points)\n",
    "            img_reconstructed_3d = img_reconstructed\n",
    "\n",
    "            result = frame, img_reconstructed_3d\n",
    "            self._cache[idx] = result\n",
    "            \n",
    "        return self._cache[idx]\n",
    "\n",
    "    def get_number_of_persons_for_frame(self, idx):\n",
    "        batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "        return len(batch.centre_points[subindex])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "training_dataset = IrPersonsUnetTrainDataset(augmented_data_training)\n",
    "validation_dataset = IrPersonsUnetTrainDataset(augmented_data_validation)\n",
    "\n",
    "\n",
    "# it makes no sense to split all data, as most of the frames are almost identical\n",
    "# training_dataset, validation_dataset = torch.utils.data.random_split(all_data_dataset, [training_data_len, validation_data_len])\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(training_dataset, batch_size=16, shuffle=True, worker_init_fn=seed_worker)\n",
    "valloader = torch.utils.data.DataLoader(validation_dataset, batch_size=16, shuffle=True, worker_init_fn=seed_worker)\n",
    "\n",
    "\n",
    "print(len(trainloader))\n",
    "print(len(valloader))\n",
    "\n",
    "\n",
    "\n",
    "xb, yb = next(iter(trainloader))\n",
    "xb.shape, yb.shape\n",
    "\n",
    "\n",
    "plt.imshow(yb[0].numpy().squeeze())\n",
    "\n",
    "print(f'number of persosn based on sum: {np.sum(yb[0].numpy()) / sum_of_values_for_one_person:.3f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sum of pixels for one person on the image\n",
    "\n",
    "# total_sum_of_pixels = 0\n",
    "# total_number_of_people = 0\n",
    "\n",
    "# for i in range(len(training_dataset)):\n",
    "#     reconstructed_frame = training_dataset[i][1]\n",
    "    \n",
    "#     sum_of_pixels = np.sum(reconstructed_frame)\n",
    "#     numbe_of_people = training_dataset.get_number_of_persons_for_frame(i)\n",
    "    \n",
    "#     plt.imshow(reconstructed_frame)\n",
    "\n",
    "#     total_sum_of_pixels += sum_of_pixels\n",
    "#     total_number_of_people += numbe_of_people\n",
    "\n",
    "\n",
    "# average_pixels_per_person = total_sum_of_pixels / total_number_of_people\n",
    "# print(f'average_pixels_per_person={average_pixels_per_person}')  # 51.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMIeE34BVxV9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(loader, model):\n",
    "    correct_count = 0\n",
    "    tested_frames = 0\n",
    "    number_of_frames_with_n_persons = {}\n",
    "    number_of_frames_with_n_persons_predicted_correctly = {}\n",
    "\n",
    "    persons_error_sum = 0\n",
    "\n",
    "    for frame, labels in loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(frame.to(device)).to(cpu_device)\n",
    "            \n",
    "        for i in range(len(labels)):\n",
    "            predicted_img = outputs[i].numpy()\n",
    "\n",
    "            pred_people = np.sum(predicted_img) / sum_of_values_for_one_person\n",
    "            pred_label = round(pred_people)\n",
    "\n",
    "            true_label = round(np.sum(labels.numpy()[i]) / sum_of_values_for_one_person)  # not true entirely, but good enough for testing. One would need to obtain real number of people on image\n",
    "\n",
    "            persons_error_sum += abs(pred_people - true_label)\n",
    "\n",
    "            number_of_frames_with_n_persons[pred_label] = \\\n",
    "                number_of_frames_with_n_persons.get(pred_label, 0) + 1\n",
    "\n",
    "            if true_label == pred_label:\n",
    "                correct_count += 1\n",
    "                number_of_frames_with_n_persons_predicted_correctly[pred_label] = \\\n",
    "                    number_of_frames_with_n_persons_predicted_correctly.get(pred_label, 0) + 1\n",
    "\n",
    "            tested_frames += 1\n",
    "\n",
    "    average_prediction_error = persons_error_sum / tested_frames\n",
    "    model_accuracy = correct_count / tested_frames\n",
    "    \n",
    "    print(f\"Number of tested frames: {tested_frames}\")\n",
    "    print(f\"Model Accuracy = {model_accuracy}\")\n",
    "    print('Predicted:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons.items()]))\n",
    "    print('Predicted correctly:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons_predicted_correctly.items()]))\n",
    "    print(f'average_prediction_error: {average_prediction_error}')\n",
    "    \n",
    "    return model_accuracy, average_prediction_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "451QAM2vVxYy",
    "outputId": "c3a77c48-bc33-4920-d9e0-f2c9cd1c7233"
   },
   "outputs": [],
   "source": [
    "xb, yb = next(iter(trainloader))\n",
    "xb = xb.to(device)\n",
    "\n",
    "xb.shape\n",
    "\n",
    "\n",
    "# unet(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtsKqO83Vxbu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMyO17JCXBhl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgDr3updXBkn",
    "outputId": "d748c151-67d6-4a30-da15-35742e4f35b1"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_channels)\n",
    "        self.conv = DoubleConv(64, 128, 3, 1)\n",
    "        self.upconv1 = ExpandBlock(128, 64, 3, 1)\n",
    "        self.upconv2 = ExpandBlock(64, 32, 3, 1)\n",
    "        self.out_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        # downsampling part\n",
    "        x, conv2, conv1 = self.encoder(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.upconv1(x, conv2)\n",
    "        x = self.upconv2(x, conv1)\n",
    "        x = self.out_conv(x)\n",
    "        \n",
    "        x = x[:, 0, :, :]  # get rid of one dimension\n",
    "        return x\n",
    "\n",
    "\n",
    "class ExpandBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, padding: int):\n",
    "        super().__init__()\n",
    "        self.conv_transpose = nn.ConvTranspose2d(\n",
    "            in_channels, in_channels // 2, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x: torch.Tensor, encoder_features: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_transpose(x)\n",
    "        x = torch.cat((x, encoder_features), dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = ContractBlock(in_channels, 32, 3, 1)\n",
    "        self.conv2 = ContractBlock(32, 64, 3, 1)\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x, conv1 = self.conv1(x)\n",
    "        x, conv2 = self.conv2(x)\n",
    "        return x, conv2, conv1\n",
    "    def forward_simple(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_conv(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ContractBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        features = self.conv(x)\n",
    "        x = self.pool(features)\n",
    "        return x, features\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Sequential):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, padding: int = 0):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "unet = AutoEncoder(1, 1).double()\n",
    "unet = unet.to(device)\n",
    "\n",
    "\n",
    "unet.encoder.load_state_dict(torch.load(pretrained_encoder_file_path))\n",
    "\n",
    "unet.encoder.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNdMx1-DXBnx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t0s9PuASOWti",
    "outputId": "0fe207a6-77c6-40c0-f604-64b44b9b200b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_dl, valid_dl, loss_fn, optimizer, epochs=1):\n",
    "    start = time.time()\n",
    "    train_loss, valid_loss, valid_error, accuracy_vec = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set trainind mode = true\n",
    "                dataloader = train_dl\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = valid_dl\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            step = 0\n",
    "            for x, y in dataloader:\n",
    "                if step != 0:\n",
    "                    if randrange(200) != 1:  # do not train on every frame in each epoch\n",
    "                        continue\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = loss_fn(outputs, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(x)\n",
    "                        loss = loss_fn(outputs, y)\n",
    "                                \n",
    "\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "            epoch_loss = running_loss / step\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}. lr={lr}')\n",
    "            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n",
    "            \n",
    "        \n",
    "        # Plot one frame\n",
    "        with torch.no_grad():\n",
    "            example_frames = next(iter(trainloader))[0].to(device)\n",
    "            model_ouput_frames = model(example_frames).to(cpu_device)\n",
    "            model_ouput_frame = model_ouput_frames[0]\n",
    "            plt.imshow(model_ouput_frame)\n",
    "\n",
    "\n",
    "        # full validation of the model\n",
    "        accuracy, prediction_error = validate_model(loader=valid_dl, model=model)\n",
    "        valid_error.append(prediction_error)\n",
    "        accuracy_vec.append(accuracy)\n",
    "            \n",
    "#         if epoch > 30 and valid_error and valid_error[-1] < 0.6:  # TODO - different value for different validation data\n",
    "#             print('Training finished, results good enough...')\n",
    "#             break\n",
    "            \n",
    "            \n",
    "    time_elapsed = time.time() - start\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')    \n",
    "    return train_loss, valid_loss, valid_error, accuracy_vec    \n",
    "\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "# loss_fn = nn.MSELoss()  # sometimes learn strangely\n",
    "\n",
    "opt = torch.optim.Adam(unet.parameters(), lr=0.001)\n",
    "train_loss, valid_loss, valid_error, accuracy_vec = train(model=unet, \n",
    "                               train_dl=trainloader, \n",
    "                               valid_dl=valloader, \n",
    "                               loss_fn=loss_fn, \n",
    "                               optimizer=opt, \n",
    "                               epochs=450)\n",
    "\n",
    "if train_loss[-1] > 0.04:\n",
    "    raise Exception(\"Training error, reinitialize weights. Network sometimes doesn't learn as it should\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "TH22T1igOWwN",
    "outputId": "330b030c-de27-450a-bc59-77960e59f3a6"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.legend(['train', 'valid'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "5A9r2tQpjQe_",
    "outputId": "75618751-8f9b-4651-e6f0-c89b2cc60efd"
   },
   "outputs": [],
   "source": [
    "plt.plot(valid_error)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('valid_error')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_vec)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy_vec')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KjDvwNTgOWzE",
    "outputId": "26225c87-a6b9-46d5-8a43-ffa34b8e30bd"
   },
   "outputs": [],
   "source": [
    "validate_model(loader=valloader, model=unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WU832ZLHQMwC"
   },
   "outputs": [],
   "source": [
    "# PATH = \"/home/przemek/Projects/thermo-presence/thermo-presence/data_collection/src/trained_model/unet_v2_cpu1\"\n",
    "\n",
    "\n",
    "# unet_cpu = unet.to(cpu_device)\n",
    "# torch.save(unet_cpu.state_dict(), PATH)\n",
    "\n",
    "\n",
    "# unet.load_state_dict(torch.load(PATH))\n",
    "# unet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7n5KyoOEQMyv"
   },
   "outputs": [],
   "source": [
    "class IrPersonsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, augmented_data: AugmentedBatchesTrainingData, transform=None):\n",
    "        self.augmented_data = AugmentedBatchesTrainingData\n",
    "        self.transform = transform\n",
    "        self._index_to_batch_and_subindex_map = {}\n",
    "        \n",
    "        i = 0\n",
    "        for batch in augmented_data.batches:\n",
    "            for j in range(len(batch.raw_ir_data)):\n",
    "                self._index_to_batch_and_subindex_map[i] = (batch, j) \n",
    "                i += 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len (self._index_to_batch_and_subindex_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            raise Exception(\"Not supported\")\n",
    "\n",
    "        batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "        frame = batch.normalized_ir_data[subindex][np.newaxis, :, :]\n",
    "        return frame, len(batch.centre_points[subindex])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlQa4AUxQM1S"
   },
   "outputs": [],
   "source": [
    "VALIDATION_DIRS_2 = VALIDATION_DIRS_1\n",
    "\n",
    "\n",
    "data_with_people_cound = load_data_for_labeled_batches(labeled_batch_dirs=VALIDATION_DIRS_2)\n",
    "\n",
    "augmented_data_with_people_count = AugmentedBatchesTrainingData()\n",
    "augmented_data_with_people_count.add_training_batch(data_with_people_cound, flip_and_rotate=False)\n",
    "\n",
    "dataset_with_real_people_count = IrPersonsDataset(augmented_data_with_people_count)\n",
    "loader_with_real_people_count = torch.utils.data.DataLoader(dataset_with_real_people_count, batch_size=1, shuffle=False, worker_init_fn=seed_worker)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bQu1fIdgQM5G",
    "outputId": "035d3742-a414-4c3f-f951-ed2ea60956c6"
   },
   "outputs": [],
   "source": [
    "def validate_model_with_real_number_of_persons(loader, model, data_plotting_interval=1000):\n",
    "    correct_count = 0\n",
    "    tested_frames = 0\n",
    "    number_of_frames_with_n_persons = {}\n",
    "    number_of_frames_with_n_persons_predicted_correctly = {}\n",
    "\n",
    "    persons_error_sum = 0\n",
    "    \n",
    "    vec_real_number_of_persons = []\n",
    "    vec_predicted_number_of_persons = []\n",
    "\n",
    "    for frame, labels in loader:\n",
    "        for i in range(len(labels)):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(frame.to(device)).to(cpu_device)\n",
    "            predicted_img = outputs[i].numpy()\n",
    "\n",
    "            pred_people = np.sum(predicted_img) / sum_of_values_for_one_person\n",
    "            pred_label = round(pred_people)\n",
    "\n",
    "            true_label = labels.numpy()[i]\n",
    "\n",
    "            persons_error_sum += abs(pred_people - true_label)\n",
    "\n",
    "            number_of_frames_with_n_persons[pred_label] = \\\n",
    "                number_of_frames_with_n_persons.get(pred_label, 0) + 1\n",
    "\n",
    "            if true_label == pred_label:\n",
    "                correct_count += 1\n",
    "                number_of_frames_with_n_persons_predicted_correctly[pred_label] = \\\n",
    "                    number_of_frames_with_n_persons_predicted_correctly.get(pred_label, 0) + 1\n",
    "\n",
    "            tested_frames += 1\n",
    "            \n",
    "            vec_real_number_of_persons.append(true_label)\n",
    "            vec_predicted_number_of_persons.append(pred_people)\n",
    "            \n",
    "            if tested_frames % data_plotting_interval == 0:\n",
    "                plt.imshow(frame[i, 0, :, :])\n",
    "                plt.show()\n",
    "                print(f'true_label={true_label}, pred_people={pred_people}')\n",
    "                plt.imshow(predicted_img)\n",
    "                plt.show()\n",
    "                print('#'*30)\n",
    "\n",
    "\n",
    "    average_prediction_error = persons_error_sum / tested_frames\n",
    "    model_accuracy = correct_count / tested_frames\n",
    "    \n",
    "    print(f\"Number of tested frames: {tested_frames}\")\n",
    "    print(f\"Model Accuracy = {model_accuracy}\")\n",
    "    print('Predicted:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons.items()]))\n",
    "    print('Predicted correctly:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons_predicted_correctly.items()]))\n",
    "    print(f'average_prediction_error: {average_prediction_error}')\n",
    "    \n",
    "    return model_accuracy, average_prediction_error, vec_real_number_of_persons, vec_predicted_number_of_persons\n",
    "\n",
    "\n",
    "_, _, real_vec, predicted_vec = validate_model_with_real_number_of_persons(loader=loader_with_real_people_count, model=unet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "7L3aCk-7QM7z",
    "outputId": "103d2b22-b029-4301-d70d-6f718a5ea961"
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(8,6), dpi= 100, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(real_vec, linewidth=2.5)\n",
    "plt.plot(predicted_vec, linewidth=1)\n",
    "\n",
    "plt.title('number of people')\n",
    "plt.legend(['real', 'predicted'])\n",
    "plt.xlabel('time [0.5*s]')\n",
    "plt.ylabel('people count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zboRO37CQM9c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "image_segmentation_unet_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
