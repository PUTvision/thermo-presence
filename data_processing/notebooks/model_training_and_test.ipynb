{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaTSxqAkIp3l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time \n",
    "from random import randrange\n",
    "import json\n",
    "import copy\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nxn6v3wqYI4I",
    "outputId": "98d091fb-c4e9-4afc-d7b0-3f62fa738b83"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cpu_device = torch.device('cpu')\n",
    "device  # device to use for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data files path and dataset definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gBEE8_EYJZW",
    "outputId": "2930569a-be51-4df6-f5ca-358193f7c1f4"
   },
   "outputs": [],
   "source": [
    "PROJECT_DIR_PATH = '.'\n",
    "DATA_DIR_PATH = os.path.join(PROJECT_DIR_PATH, 'data')\n",
    "LABELS_DIR = os.path.join(PROJECT_DIR_PATH, 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cp2paP8NYJkS"
   },
   "outputs": [],
   "source": [
    "# Directories with data to be used for each dataset\n",
    "\n",
    "TRAINING_DIRS_1 = [\n",
    "    '006__11_44_59',\n",
    "    '007__11_48_59',\n",
    "    '008__11_52_59',\n",
    "    '009__11_57_00',\n",
    "     \n",
    "    '000__14_15_19',\n",
    "    '001__14_19_19',\n",
    "    '002__14_23_19',\n",
    "    '003__14_27_20',\n",
    "    '004__14_31_20',\n",
    "    \n",
    "    '012__15_03_21',\n",
    "    '013__15_07_21',\n",
    "    '014__15_11_21',\n",
    "    '015__15_15_21',\n",
    "    '016__15_19_21',\n",
    "    \n",
    "    '011__13_38_20',\n",
    "    '012__13_42_20',\n",
    "    '013__13_46_21',\n",
    "    \n",
    "    '007__13_22_20',\n",
    "    ]\n",
    "\n",
    "\n",
    "VALIDATION_DIRS_1 = [\n",
    "    '004__13_10_20',\n",
    "\n",
    "    '014__13_50_21',\n",
    "    \n",
    "    '005__14_35_20',\n",
    "    '006__14_39_20',\n",
    "    '007__14_43_20',\n",
    "    '008__14_47_20',\n",
    "]\n",
    "\n",
    "\n",
    "TEST_DIRS_1 = [\n",
    "    '008__13_26_20',\n",
    "    \n",
    "    '009__14_51_20',\n",
    "    '010__14_55_20',\n",
    "    '011__14_59_20',\n",
    "\n",
    "    '015__13_54_21',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous definitions and data reading functions\n",
    "\n",
    "(nothing really interesting in this section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfiVqcRvJB-E"
   },
   "outputs": [],
   "source": [
    "IR_CAMERA_RESOLUTION_X = 32\n",
    "IR_CAMERA_RESOLUTION_Y = 24\n",
    "\n",
    "IR_CAMERA_RESOLUTION = (IR_CAMERA_RESOLUTION_Y, IR_CAMERA_RESOLUTION_X)\n",
    "\n",
    "IR_CAMERA_RESOLUTION_XY = (IR_CAMERA_RESOLUTION_X, IR_CAMERA_RESOLUTION_Y)  # for opencv functions\n",
    "\n",
    "# for frames normalization\n",
    "TEMPERATURE_NORMALIZATION__MIN = 20\n",
    "TEMPERATURE_NORMALIZATION__MAX = 35\n",
    "\n",
    "IR_FRAME_RESIZE_MULTIPLIER = 1\n",
    "\n",
    "MIN_TEMPERATURE_ON_PLOT = 20  # None for auto range\n",
    "MAX_TEMPERATURE_ON_PLOT = 30  # None for auto range\n",
    "IR_FRAME_INTERPOLATION_METHOD = cv2.INTER_CUBIC\n",
    "RGB_FRAME_RESIZE_MULTIPLIER = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "375jtwrkJcs1"
   },
   "outputs": [],
   "source": [
    "class IrDataCsvReader:\n",
    "    def __init__(self, file_path):\n",
    "        self._file_path = file_path\n",
    "        self._frames, self._raw_frame_data = self.get_frames_from_file(file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_frames_from_file(file_path):\n",
    "        frames = []\n",
    "        raw_frame_data = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            raw_lines = file.readlines()\n",
    "        lines = [x.strip() for x in raw_lines]\n",
    "        frame_lines = lines[1:]\n",
    "        for frame_line in frame_lines:\n",
    "            raw_frame_data.append(frame_line)\n",
    "            line_parts = frame_line.split(',')\n",
    "            frame_data_str = line_parts[2:]\n",
    "            frame_data_1d = [float(x) for x in frame_data_str]\n",
    "            frame_2d = np.reshape(frame_data_1d, IR_CAMERA_RESOLUTION)\n",
    "            frames.append(frame_2d)\n",
    "        return frames, raw_frame_data\n",
    "\n",
    "    def get_number_of_frames(self):\n",
    "        return len(self._frames)\n",
    "\n",
    "    def get_frame(self, n):\n",
    "        return self._frames[n]\n",
    "\n",
    "    def get_raw_frame_data(self, n) -> str:\n",
    "        return self._raw_frame_data[n]\n",
    "\n",
    "\n",
    "\n",
    "class BatchTrainingData:\n",
    "    \"\"\"\n",
    "    Stores training data for one batch\n",
    "    \"\"\"\n",
    "    def __init__(self, min_temperature=TEMPERATURE_NORMALIZATION__MIN, max_temperature=TEMPERATURE_NORMALIZATION__MAX):\n",
    "        self.centre_points = []  # type: List[List[tuple]]\n",
    "        self.raw_ir_data = []  # type: List[np.ndarray]\n",
    "        self.normalized_ir_data = []  # type: List[np.ndarray]  # same data as raw_ir_data, but normalized\n",
    "\n",
    "        self.min_temperature = min_temperature\n",
    "        self.max_temperature = max_temperature\n",
    "\n",
    "    def append_frame_data(self, centre_points, raw_ir_data):\n",
    "        self.centre_points.append(centre_points)\n",
    "        self.raw_ir_data.append(raw_ir_data)\n",
    "\n",
    "        ir_data_normalized = (raw_ir_data - self.min_temperature) * (1 / (self.max_temperature - self.min_temperature))\n",
    "        self.normalized_ir_data.append(ir_data_normalized)\n",
    "\n",
    "    def flip_horizontally(self):\n",
    "        for i in range(len(self.raw_ir_data)):\n",
    "            self.raw_ir_data[i] = np.flip(self.raw_ir_data[i], 1)\n",
    "            self.normalized_ir_data[i] = np.flip(self.normalized_ir_data[i], 1)\n",
    "            for j in range(len(self.centre_points[i])):\n",
    "                x_flipped = IR_CAMERA_RESOLUTION_X - self.centre_points[i][j][0]\n",
    "                self.centre_points[i][j] = (x_flipped, self.centre_points[i][j][1])\n",
    "\n",
    "    def flip_vertically(self):\n",
    "        for i in range(len(self.raw_ir_data)):\n",
    "            self.raw_ir_data[i] = np.flip(self.raw_ir_data[i], 0)\n",
    "            self.normalized_ir_data[i] = np.flip(self.normalized_ir_data[i], 0)\n",
    "            for j in range(len(self.centre_points[i])):\n",
    "                y_flipped = IR_CAMERA_RESOLUTION_Y - self.centre_points[i][j][1]\n",
    "                self.centre_points[i][j] = (self.centre_points[i][j][0], y_flipped)\n",
    "\n",
    "\n",
    "class AugmentedBatchesTrainingData:\n",
    "    \"\"\"\n",
    "    Stores training data for all batches, with data augmentation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.batches = []  # Type: List[BatchTrainingData]\n",
    "\n",
    "    def add_training_batch(self, batch: BatchTrainingData, flip_and_rotate=True):\n",
    "        self.batches.append(copy.deepcopy(batch))  # plain data\n",
    "\n",
    "        if flip_and_rotate:\n",
    "            batch.flip_horizontally()\n",
    "            self.batches.append(copy.deepcopy(batch))  # flipped horizontally\n",
    "\n",
    "            batch.flip_vertically()\n",
    "            self.batches.append(copy.deepcopy(batch))  # rotated 180 degrees\n",
    "\n",
    "            batch.flip_horizontally()\n",
    "            self.batches.append(copy.deepcopy(batch))  # rotated vertically\n",
    "\n",
    "    def print_stats(self):\n",
    "        total_number_of_frames = 0\n",
    "        number_of_frames_with_n_persons = {}\n",
    "        for batch in self.batches:\n",
    "            total_number_of_frames += len(batch.centre_points)\n",
    "            for centre_points in batch.centre_points:\n",
    "                number_of_persons = len(centre_points)\n",
    "                number_of_frames_with_n_persons[number_of_persons] = \\\n",
    "                    number_of_frames_with_n_persons.get(number_of_persons, 0) + 1\n",
    "        frames_persons_details_msg = '\\n'.join([f'   {count} frames with {no} persons'\n",
    "                                               for no, count in number_of_frames_with_n_persons.items()])\n",
    "        msg = f\"AugmentedBatchesTrainingData with {len(self.batches)} BatchTrainingData batches.\\n\" \\\n",
    "              f\"Total number of frames after augmentation: {total_number_of_frames}, with:\\n\" \\\n",
    "              f\"{frames_persons_details_msg}\"\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def load_data_for_labeled_batches(labeled_batch_dirs) -> BatchTrainingData:\n",
    "\n",
    "    training_data = BatchTrainingData()\n",
    "    for batch_subdir in labeled_batch_dirs:\n",
    "        data_batch_dir_path = os.path.join(DATA_DIR_PATH, batch_subdir)\n",
    "        raw_ir_data_csv_file_path = os.path.join(data_batch_dir_path, 'ir.csv')\n",
    "        output_file_with_labels_name = batch_subdir.replace('/', '--') + '.csv'\n",
    "        annotation_data_file_path = os.path.join(LABELS_DIR, output_file_with_labels_name)\n",
    "\n",
    "        raw_ir_data_csv_reader = IrDataCsvReader(file_path=raw_ir_data_csv_file_path)\n",
    "        annotations_collector = AnnotationCollector.load_from_file(\n",
    "\n",
    "            file_path=annotation_data_file_path, do_not_scale_and_reverse=True)\n",
    "\n",
    "        for frame_index in range(raw_ir_data_csv_reader.get_number_of_frames()):\n",
    "            raw_frame_data = raw_ir_data_csv_reader.get_frame(frame_index)\n",
    "            frame_annotations = annotations_collector.get_annotation(frame_index)\n",
    "            if not frame_annotations.accepted:\n",
    "                print(f\"Frame index {frame_index} from batch '{batch_subdir}' not annotated!\")\n",
    "                continue\n",
    "            training_data.append_frame_data(\n",
    "                centre_points=frame_annotations.centre_points,\n",
    "                raw_ir_data=raw_frame_data)\n",
    "\n",
    "    return training_data\n",
    "\n",
    "\n",
    "\n",
    "class AnnotationCollector:\n",
    "    ANNOTATIONS_BETWEEN_AUTOSAVE = 10\n",
    "\n",
    "    def __init__(self, output_file_path, data_batch_dir_path):\n",
    "        self._output_file_path = output_file_path\n",
    "        self._annotations = {}  # ir_frame_index: FrameAnnotation\n",
    "        self._data_batch_dir_path = data_batch_dir_path\n",
    "        self._annotations_to_autosave = 1\n",
    "\n",
    "    def get_annotation(self, ir_frame_index):\n",
    "        return self._annotations.get(ir_frame_index, FrameAnnotation())\n",
    "\n",
    "    def set_annotation(self, ir_frame_index, annotation):\n",
    "        self._annotations[ir_frame_index] = annotation\n",
    "        if annotation.accepted or annotation.discarded:\n",
    "            self._annotations_to_autosave -= 1\n",
    "            if self._annotations_to_autosave == 0:\n",
    "                self._annotations_to_autosave = self.ANNOTATIONS_BETWEEN_AUTOSAVE\n",
    "                self.save()\n",
    "\n",
    "    def save(self):\n",
    "        data_dict = {\n",
    "            'output_file_path': self._output_file_path,\n",
    "            'data_batch_dir_path': self._data_batch_dir_path,\n",
    "            'annotations': {index: annotation.as_dict()\n",
    "                            for index, annotation in self._annotations.items()}\n",
    "        }\n",
    "        with open(self._output_file_path, 'w') as file:\n",
    "            file.write(json.dumps(data_dict, indent=2))\n",
    "            file.flush()\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_file(cls, file_path, do_not_scale_and_reverse=False):\n",
    "        item = cls(output_file_path=file_path, data_batch_dir_path=None)\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.read()\n",
    "        data_dict = json.loads(data)\n",
    "        item._data_batch_dir_path = data_dict['data_batch_dir_path']\n",
    "        item._annotations = {int(index): FrameAnnotation.from_dict(annotation_dict, do_not_scale_and_reverse) for index, annotation_dict\n",
    "                             in data_dict['annotations'].items()}\n",
    "        return item\n",
    "\n",
    "\n",
    "def x_on_interpolated_image_to_raw_x(x):\n",
    "    x_raw_flipped = x / IR_FRAME_RESIZE_MULTIPLIER\n",
    "    x_raw = IR_CAMERA_RESOLUTION_X - x_raw_flipped\n",
    "    return x_raw\n",
    "\n",
    "\n",
    "def y_on_interpolated_image_to_raw_y(y):\n",
    "    return y / IR_FRAME_RESIZE_MULTIPLIER\n",
    "\n",
    "\n",
    "def xy_on_interpolated_image_to_raw_xy(xy: tuple) -> tuple:\n",
    "    return (x_on_interpolated_image_to_raw_x(xy[0]),\n",
    "            y_on_interpolated_image_to_raw_y(xy[1]))\n",
    "\n",
    "\n",
    "class FrameAnnotation:\n",
    "    def __init__(self):\n",
    "        self.accepted = False  # whether frame was marked as annotated successfully\n",
    "        self.discarded = False  # whether frame was marked as discarded (ignored)\n",
    "        self.centre_points = []  # type: List[tuple]  # x, y\n",
    "        self.rectangles = []  # type: List[Tuple[tuple, tuple]]  # (x_left, y_top), (x_right, y_bottom)\n",
    "\n",
    "        self.raw_frame_data = None  # Not an annotation, but write it to the result file, just in case\n",
    "\n",
    "    def as_dict(self):\n",
    "        data_dict = copy.copy(self.__dict__)\n",
    "        data_dict['centre_points'] = []\n",
    "        data_dict['rectangles'] = []\n",
    "\n",
    "        for i, point in enumerate(self.centre_points):\n",
    "            data_dict['centre_points'].append(xy_on_interpolated_image_to_raw_xy(point))\n",
    "        for i, rectangle in enumerate(self.rectangles):\n",
    "            data_dict['rectangles'].append((xy_on_interpolated_image_to_raw_xy(rectangle[0]),\n",
    "                                            xy_on_interpolated_image_to_raw_xy(rectangle[1])))\n",
    "        return data_dict\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data_dict, do_not_scale_and_reverse=False):\n",
    "        item = cls()\n",
    "        item.__dict__.update(data_dict)\n",
    "        for i, point in enumerate(item.centre_points):\n",
    "            item.centre_points[i] = xy_on_raw_image_to_xy_on_interpolated_image(point, do_not_scale_and_reverse)\n",
    "        for i, rectangle in enumerate(item.rectangles):\n",
    "            item.rectangles[i] = (xy_on_raw_image_to_xy_on_interpolated_image(rectangle[0], do_not_scale_and_reverse),\n",
    "                                  xy_on_raw_image_to_xy_on_interpolated_image(rectangle[1], do_not_scale_and_reverse))\n",
    "        return item\n",
    "\n",
    "\n",
    "def x_on_interpolated_image_to_raw_x(x):\n",
    "    x_raw_flipped = x / IR_FRAME_RESIZE_MULTIPLIER\n",
    "    x_raw = IR_CAMERA_RESOLUTION_X - x_raw_flipped\n",
    "    return x_raw\n",
    "\n",
    "\n",
    "def y_on_interpolated_image_to_raw_y(y):\n",
    "    return y / IR_FRAME_RESIZE_MULTIPLIER\n",
    "\n",
    "\n",
    "def xy_on_interpolated_image_to_raw_xy(xy: tuple) -> tuple:\n",
    "    return (x_on_interpolated_image_to_raw_x(xy[0]),\n",
    "            y_on_interpolated_image_to_raw_y(xy[1]))\n",
    "\n",
    "\n",
    "def x_on_raw_image_to_x_on_interpolated_image(x):\n",
    "    x_flipped = IR_CAMERA_RESOLUTION_X - x\n",
    "    return round(x_flipped * IR_FRAME_RESIZE_MULTIPLIER)\n",
    "\n",
    "\n",
    "def y_on_raw_image_to_y_on_interpolated_image(y):\n",
    "    return round(y * IR_FRAME_RESIZE_MULTIPLIER)\n",
    "\n",
    "\n",
    "def xy_on_raw_image_to_xy_on_interpolated_image(xy: tuple, do_not_scale_and_reverse=False) -> tuple:\n",
    "    if do_not_scale_and_reverse:\n",
    "        return xy\n",
    "\n",
    "    return (x_on_raw_image_to_x_on_interpolated_image(xy[0]),\n",
    "            y_on_raw_image_to_y_on_interpolated_image(xy[1]))\n",
    "\n",
    "\n",
    "def get_extrapolated_ir_frame_heatmap_flipped(\n",
    "        frame_2d, multiplier, interpolation, min_temp, max_temp, colormap):\n",
    "    new_size = (frame_2d.shape[1] * multiplier, frame_2d.shape[0] * multiplier)\n",
    "    frame_resized_not_clipped = cv2.resize(\n",
    "        src=frame_2d, dsize=new_size, interpolation=interpolation)\n",
    "\n",
    "    if min_temp is None:\n",
    "        min_temp = min(frame_resized_not_clipped.reshape(-1))\n",
    "    if max_temp is None:\n",
    "        max_temp = max(frame_resized_not_clipped.reshape(-1))\n",
    "\n",
    "    frame_resized = np.clip(frame_resized_not_clipped, min_temp, max_temp)\n",
    "    frame_resized_normalized = (frame_resized - min_temp) * (255 / (max_temp - min_temp))\n",
    "    frame_resized_normalized_u8 = frame_resized_normalized.astype(np.uint8)\n",
    "    heatmap_u8 = (colormap(frame_resized_normalized_u8) * 2 ** 8).astype(np.uint8)[:, :, :3]\n",
    "    heatmap_u8_bgr = cv2.cvtColor(heatmap_u8, cv2.COLOR_RGB2BGR)\n",
    "    heatmap_u8_bgr_flipped = cv2.flip(heatmap_u8_bgr, 1)  # horizontal flip\n",
    "    return heatmap_u8_bgr_flipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_airbrush_circle(img, centre, radius):\n",
    "    for x in range(max(0, round(centre[0]-radius)), min(img.shape[0], round(centre[0]+radius+1))):\n",
    "        for y in range(max(0, round(centre[1]-radius)), min(img.shape[1], round(centre[1]+radius+1))):\n",
    "            point = (x, y)\n",
    "            distance_to_centre = cv2.norm((centre[0] - x, centre[1] - y))\n",
    "            if distance_to_centre > radius:\n",
    "                continue\n",
    "            img[point] += 1 - distance_to_centre / radius\n",
    "            \n",
    "\n",
    "def draw_cross(img, centre, cross_width, cross_height):\n",
    "    for x in range(max(0, round(centre[0]) - cross_width), min(img.shape[0], round(centre[0]) + cross_width + 1)):\n",
    "        for y in range(max(0, round(centre[1]) - cross_height), min(img.shape[1], round(centre[1]) + cross_height + 1)):\n",
    "            point = (x, y)\n",
    "            img[point] = 1\n",
    "    \n",
    "    for x in range(max(0, round(centre[0] - cross_height)), min(img.shape[0], round(centre[0] + cross_height + 1))):\n",
    "        for y in range(max(0, round(centre[1] - cross_width)), min(img.shape[1], round(centre[1] + cross_width + 1))):\n",
    "            point = (x, y)\n",
    "            img[point] = 1\n",
    "            \n",
    "\n",
    "def gauss_1d(x, sig):\n",
    "    return np.exp(-np.power(x, 2.) / (2 * np.power(sig, 2.)))\n",
    "    \n",
    "    \n",
    "def draw_gauss(img, sig, centre):\n",
    "    radius = 3 * sig\n",
    "    for x in range(max(0, round(centre[0]-radius)), min(img.shape[0], round(centre[0]+radius+1))):\n",
    "        for y in range(max(0, round(centre[1]-radius)), min(img.shape[1], round(centre[1]+radius+1))):\n",
    "            point = (x, y)\n",
    "            distance_to_centre = cv2.norm((centre[0] - x, centre[1] - y))\n",
    "            if distance_to_centre > radius:\n",
    "                continue\n",
    "            gauss_value = gauss_1d(distance_to_centre, sig)\n",
    "            img[point] += gauss_value\n",
    "    \n",
    "    \n",
    "def get_img_reconstructed_from_labels(centre_points):\n",
    "    \"\"\" Function used to create training data basing on the labels \"\"\"\n",
    "    \n",
    "    img_reconstructed = np.zeros(shape=(IR_CAMERA_RESOLUTION[0], \n",
    "                                        IR_CAMERA_RESOLUTION[1]))\n",
    "\n",
    "    for centre_point in centre_points:\n",
    "        centre_point = centre_point[::-1]  # reversed x and y in \n",
    "        draw_gauss(img=img_reconstructed, \n",
    "                   centre=[c for c in centre_point], \n",
    "                   sig=3)\n",
    "    \n",
    "    #img_int = (img_reconstructed * (NUMBER_OF_OUPUT_CLASSES-1)).astype('int')\n",
    "    #return img_int\n",
    "    return img_reconstructed\n",
    "\n",
    "\n",
    "# For Gauss function with sig=3, sum of values for one person is between 45 - 55, depending whether people are on the edge. \n",
    "# To predict the number of people on a frame as a sum of pixels of the predicted image, we need to divide every pixel by a constant   \n",
    "sum_of_values_for_one_person = 51.35  # total sum of pixels for one person on the reconstructed image, of course it changes with gaussian function parameters. Calculated as average from all training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AIpGLgeyJCGB",
    "outputId": "ed650752-0224-41ee-8d9d-2e764d2e8a19"
   },
   "outputs": [],
   "source": [
    "_training_data_1 = load_data_for_labeled_batches(labeled_batch_dirs=TRAINING_DIRS_1)\n",
    "_validation_data_1 = load_data_for_labeled_batches(labeled_batch_dirs=VALIDATION_DIRS_1)\n",
    "_test_data_1 = load_data_for_labeled_batches(labeled_batch_dirs=TEST_DIRS_1)\n",
    "\n",
    "augmented_data_training = AugmentedBatchesTrainingData()\n",
    "augmented_data_training.add_training_batch(_training_data_1)\n",
    "\n",
    "augmented_data_validation = AugmentedBatchesTrainingData()\n",
    "augmented_data_validation.add_training_batch(_validation_data_1, flip_and_rotate=False)\n",
    "\n",
    "augmented_data_test = AugmentedBatchesTrainingData()\n",
    "augmented_data_test.add_training_batch(_test_data_1, flip_and_rotate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZaHnpXGJCId"
   },
   "outputs": [],
   "source": [
    "augmented_data_training.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlG92cIEJCLx"
   },
   "outputs": [],
   "source": [
    "augmented_data_validation.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzWSD-zWOWn_"
   },
   "outputs": [],
   "source": [
    "augmented_data_test.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validation datasets loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "ggaNQeFDOWry",
    "outputId": "76cd705c-fbe2-4503-abbb-093b38d62ccf"
   },
   "outputs": [],
   "source": [
    "class IrPersonsUnetTrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, augmented_data: AugmentedBatchesTrainingData, transform=None):\n",
    "        self.augmented_data = AugmentedBatchesTrainingData\n",
    "        self.transform = transform\n",
    "        self._index_to_batch_and_subindex_map = {}\n",
    "        \n",
    "        self._cache = {}\n",
    "        \n",
    "        i = 0\n",
    "        for batch in augmented_data.batches:\n",
    "            for j in range(len(batch.raw_ir_data)):\n",
    "                self._index_to_batch_and_subindex_map[i] = (batch, j) \n",
    "                i += 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len (self._index_to_batch_and_subindex_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            raise Exception(\"Not supported\")\n",
    "        \n",
    "        if idx not in self._cache:\n",
    "            batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "            frame = batch.normalized_ir_data[subindex][np.newaxis, :, :]\n",
    "\n",
    "            batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "            centre_points = batch.centre_points[subindex]\n",
    "            img_reconstructed = get_img_reconstructed_from_labels(centre_points)\n",
    "            img_reconstructed_3d = img_reconstructed\n",
    "\n",
    "            result = frame, img_reconstructed_3d\n",
    "            self._cache[idx] = result\n",
    "            \n",
    "        return self._cache[idx]\n",
    "\n",
    "    def get_number_of_persons_for_frame(self, idx):\n",
    "        batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "        return len(batch.centre_points[subindex])\n",
    "        \n",
    "    \n",
    "class IrPersonsTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, augmented_data: AugmentedBatchesTrainingData, transform=None):\n",
    "        self.augmented_data = AugmentedBatchesTrainingData\n",
    "        self.transform = transform\n",
    "        self._index_to_batch_and_subindex_map = {}\n",
    "        \n",
    "        i = 0\n",
    "        for batch in augmented_data.batches:\n",
    "            for j in range(len(batch.raw_ir_data)):\n",
    "                self._index_to_batch_and_subindex_map[i] = (batch, j) \n",
    "                i += 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len (self._index_to_batch_and_subindex_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            raise Exception(\"Not supported\")\n",
    "\n",
    "        batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "        frame = batch.normalized_ir_data[subindex][np.newaxis, :, :]\n",
    "        return frame, len(batch.centre_points[subindex])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "training_dataset = IrPersonsUnetTrainDataset(augmented_data_training)\n",
    "validation_dataset = IrPersonsUnetTrainDataset(augmented_data_validation)\n",
    "\n",
    "\n",
    "# it makes no sense to split all data, as most of the frames are almost identical. Instead every dataset consists of distinctive sequences\n",
    "# training_dataset, validation_dataset = torch.utils.data.random_split(all_data_dataset, [training_data_len, validation_data_len])\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(training_dataset, batch_size=16, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(validation_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "dataset_with_real_people_count_valid = IrPersonsTestDataset(augmented_data_validation)\n",
    "loader_with_real_people_count_valid = torch.utils.data.DataLoader(dataset_with_real_people_count_valid, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "dataset_with_real_people_count_test = IrPersonsTestDataset(augmented_data_test)\n",
    "loader_with_real_people_count_test = torch.utils.data.DataLoader(dataset_with_real_people_count_test, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Example training frame:')\n",
    "xb, yb = next(iter(trainloader))\n",
    "xb.shape, yb.shape\n",
    "plt.imshow(yb[0].numpy().squeeze())\n",
    "print(f'number of persosn based on sum: {np.sum(yb[0].numpy()) / sum_of_values_for_one_person:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUq6tTXFDzWB"
   },
   "outputs": [],
   "source": [
    "# Calculate sum of pixels for one person on the image\n",
    "\n",
    "# total_sum_of_pixels = 0\n",
    "# total_number_of_people = 0\n",
    "\n",
    "# for i in range(len(training_dataset)):\n",
    "#     reconstructed_frame = training_dataset[i][1]\n",
    "    \n",
    "#     sum_of_pixels = np.sum(reconstructed_frame)\n",
    "#     numbe_of_people = training_dataset.get_number_of_persons_for_frame(i)\n",
    "    \n",
    "#     plt.imshow(reconstructed_frame)\n",
    "\n",
    "#     total_sum_of_pixels += sum_of_pixels\n",
    "#     total_number_of_people += numbe_of_people\n",
    "\n",
    "# average_pixels_per_person = total_sum_of_pixels / total_number_of_people\n",
    "# print(f'average_pixels_per_person={average_pixels_per_person}')  # 51.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMIeE34BVxV9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "451QAM2vVxYy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMyO17JCXBhl"
   },
   "source": [
    "# UNET model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgDr3updXBkn"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_channels)\n",
    "        self.conv = DoubleConv(32, 64, 3, 1)\n",
    "        self.upconv1 = ExpandBlock(64, 32, 3, 1)\n",
    "        self.upconv2 = ExpandBlock(32, 16, 3, 1)\n",
    "        self.out_conv = nn.Conv2d(16, out_channels, kernel_size=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # downsampling part\n",
    "        x, conv2, conv1 = self.encoder(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.upconv1(x, conv2)\n",
    "        x = self.upconv2(x, conv1)\n",
    "        \n",
    "        x = self.out_conv(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = x[:, 0, :, :]  # get rid of one dimension\n",
    "        return x\n",
    "\n",
    "\n",
    "class ExpandBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, padding: int):\n",
    "        super().__init__()\n",
    "        self.conv_transpose = nn.ConvTranspose2d(\n",
    "            in_channels, in_channels // 2, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x: torch.Tensor, encoder_features: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_transpose(x)\n",
    "        x = torch.cat((x, encoder_features), dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = ContractBlock(in_channels, 16, 3, 1)\n",
    "        self.conv2 = ContractBlock(16, 32, 3, 1)\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x, conv1 = self.conv1(x)\n",
    "        x, conv2 = self.conv2(x)\n",
    "        return x, conv2, conv1\n",
    "    def forward_simple(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_conv(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ContractBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        features = self.conv(x)\n",
    "        x = self.pool(features)\n",
    "        return x, features\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Sequential):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, padding: int = 0):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "unet = AutoEncoder(1, 1).double()\n",
    "unet = unet.to(device)\n",
    "\n",
    "\n",
    "\n",
    "print('Number of trainable model parameters:')\n",
    "print(sum([np.prod(p.size()) for p in filter(lambda p: p.requires_grad, unet.parameters())]))\n",
    "print(sum(p.numel() for p in unet.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNdMx1-DXBnx"
   },
   "outputs": [],
   "source": [
    "# optionally - load a pre-trained encoder\n",
    "\n",
    "# unet.encoder.load_state_dict(torch.load(pretrained_encoder_file_path))\n",
    "# unet.encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfjTmkOaOy3j"
   },
   "outputs": [],
   "source": [
    "# optionally - load trained model instead of training...\n",
    "\n",
    "# trained_model_file_path = '/home/przemek/Desktop/thermo_presence_article_files/trained_model_pytorch/unet_v2_cpu2'\n",
    "# unet.load_state_dict(torch.load(trained_model_file_path))\n",
    "# unet.eval()\n",
    "# unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9JrKH3GOy7M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mw70w9VrDSwl",
    "outputId": "18ba1964-e338-4809-b6c7-9ea1d9ec3555"
   },
   "source": [
    "# Model validation function and metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDumHCOWDSzH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_model_with_real_number_of_persons(loader, model, data_plotting_interval=3000, skip_confusion_matrix=False):\n",
    "    \"\"\" Validate the model on data from the loader, calculate and print the results and metrics \"\"\"\n",
    "    correct_count = 0\n",
    "    tested_frames = 0\n",
    "    number_of_frames_with_n_persons = {}\n",
    "    number_of_frames_with_n_persons_predicted_correctly = {}\n",
    "\n",
    "    MAX_PEOPLE_COUNT = 6\n",
    "    confusion_matrix = np.zeros(shape=(MAX_PEOPLE_COUNT+1, MAX_PEOPLE_COUNT+1), dtype=int)\n",
    "\n",
    "    mae_sum = 0\n",
    "    mse_sum = 0\n",
    "\n",
    "    mae_rounded_sum = 0\n",
    "    mse_rounded_sum = 0\n",
    "    \n",
    "    vec_real_number_of_persons = []\n",
    "    vec_predicted_number_of_persons = []\n",
    "\n",
    "    for frame, labels in loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(frame.to(device)).to(cpu_device)\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            predicted_img = outputs[i].numpy()\n",
    "\n",
    "            pred_people = np.sum(predicted_img) / sum_of_values_for_one_person\n",
    "            pred_label = round(pred_people)\n",
    "\n",
    "            true_label = labels.numpy()[i]\n",
    "\n",
    "            if not skip_confusion_matrix:\n",
    "                confusion_matrix[true_label][pred_label] += 1\n",
    "\n",
    "            error = abs(pred_people - true_label)\n",
    "            mae_sum += error\n",
    "\n",
    "\n",
    "            mse_sum += error*error\n",
    "            \n",
    "            rounded_error = abs(pred_label - true_label)\n",
    "            mae_rounded_sum += rounded_error\n",
    "            mse_rounded_sum += rounded_error*rounded_error\n",
    "\n",
    "            number_of_frames_with_n_persons[pred_label] = \\\n",
    "                number_of_frames_with_n_persons.get(pred_label, 0) + 1\n",
    "\n",
    "            if true_label == pred_label:\n",
    "                correct_count += 1\n",
    "                number_of_frames_with_n_persons_predicted_correctly[pred_label] = \\\n",
    "                    number_of_frames_with_n_persons_predicted_correctly.get(pred_label, 0) + 1\n",
    "\n",
    "            tested_frames += 1\n",
    "            \n",
    "            vec_real_number_of_persons.append(true_label)\n",
    "            vec_predicted_number_of_persons.append(pred_people)\n",
    "            \n",
    "            if tested_frames % data_plotting_interval == 0 and data_plotting_interval > 0:\n",
    "                #fig=plt.figure(figsize=(6,4), dpi= 150, facecolor='w', edgecolor='k')\n",
    "                plt.imshow(frame[i, 0, :, :])\n",
    "                plt.show()\n",
    "                print(f'true_label={true_label}, pred_people={pred_people}')\n",
    "                #fig=plt.figure(figsize=(6,4), dpi= 150, facecolor='w', edgecolor='k')\n",
    "                plt.imshow(predicted_img)\n",
    "                plt.show()\n",
    "                print('#'*30)\n",
    "\n",
    "\n",
    "    mae = mae_sum / tested_frames\n",
    "    mse = mse_sum / tested_frames\n",
    "    mae_rounded = mae_rounded_sum / tested_frames\n",
    "    mse_rounded = mse_rounded_sum / tested_frames\n",
    "\n",
    "    model_accuracy = correct_count / tested_frames\n",
    "    \n",
    "    print(f\"Number of tested frames: {tested_frames}\")\n",
    "    print(f\"Model Accuracy = {model_accuracy}\")\n",
    "    print('Predicted:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons.items()]))\n",
    "    print('Predicted correctly:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons_predicted_correctly.items()]))\n",
    "    print(f'mae: {mae}')\n",
    "    print(f'mse: {mse}')\n",
    "    print(f'mae_rounded: {mae_rounded}')\n",
    "    print(f'mse_rounded: {mse_rounded}')\n",
    "    \n",
    "    \n",
    "    return model_accuracy, mae, vec_real_number_of_persons, vec_predicted_number_of_persons, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9W13jqPgDS14"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0s9PuASOWti",
    "outputId": "7e8ad902-d3fe-4575-98a8-1633fbfb29a6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_dl, valid_dl_train, valid_dl_real, loss_fn, optimizer, epochs=1):\n",
    "    best_mae_model = None\n",
    "    best_mae = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss, valid_loss, mae_vec, accuracy_vec = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train(True)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        step = 0\n",
    "        for x, y in train_dl:\n",
    "            if step != 0:\n",
    "                if randrange(100) != 1:  # do not train on every frame in each epoch, just one in 100 frames\n",
    "                    continue\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        epoch_loss = running_loss / step\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Train Loss: {epoch_loss:.4f}. lr={lr}')\n",
    "        train_loss.append(epoch_loss)\n",
    "        \n",
    "        model.train(False)\n",
    "        \n",
    "        \n",
    "        # Plot one frame\n",
    "        with torch.no_grad():\n",
    "            example_frames = next(iter(trainloader))[0].to(device)\n",
    "            model_ouput_frames = model(example_frames).to(cpu_device)\n",
    "            model_ouput_frame = model_ouput_frames[0]\n",
    "            plt.imshow(model_ouput_frame)\n",
    "            plt.show()\n",
    "            pred = np.sum(model_ouput_frame.numpy()) / sum_of_values_for_one_person\n",
    "            print(pred)\n",
    "\n",
    "\n",
    "        # full validation of the model\n",
    "        accuracy, mae, _, _, _,  = validate_model_with_real_number_of_persons(loader=valid_dl_real, model=unet, skip_confusion_matrix=True, data_plotting_interval=-1)\n",
    "        mae_vec.append(mae)\n",
    "        accuracy_vec.append(accuracy)\n",
    "\n",
    "        if best_mae is None or mae < best_mae:\n",
    "            print('New best model saved!')\n",
    "            best_mae = mae\n",
    "            best_mae_model = copy.deepcopy(model)\n",
    "        \n",
    "\n",
    "        step = 0\n",
    "        running_loss = 0\n",
    "        for frame, labels in valid_dl_train:\n",
    "          with torch.no_grad():\n",
    "              step += 1\n",
    "              outputs = model(frame.to(device))\n",
    "              if loss_fn:\n",
    "                  loss = loss_fn(outputs, labels.to(device))\n",
    "                  running_loss += loss.item()\n",
    "        \n",
    "        single_valid_loss = running_loss / step\n",
    "        valid_loss.append(single_valid_loss)\n",
    "        print(f'Valid loss: {single_valid_loss}')\n",
    "            \n",
    "            \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'Training time: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')    \n",
    "    return train_loss, valid_loss, mae_vec, accuracy_vec, best_mae_model\n",
    "\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "# loss_fn = nn.MSELoss()  # sometimes learns strangely\n",
    "\n",
    "\n",
    "opt = torch.optim.Adam(unet.parameters(), lr=0.00001)\n",
    "train_loss, valid_loss, mae_vec, accuracy_vec, unet = train(\n",
    "    model=unet, \n",
    "    train_dl=trainloader, \n",
    "    valid_dl_train=valloader, \n",
    "    valid_dl_real=loader_with_real_people_count_valid, \n",
    "    loss_fn=loss_fn, \n",
    "    optimizer=opt, \n",
    "    epochs=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mnvtQVFLSKn",
    "outputId": "a5de8551-7021-4d75-a2aa-14d091c2c72a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "TH22T1igOWwN",
    "outputId": "5af2764e-1ce2-4834-91df-21771a008de1"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.legend(['train', 'valid'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "5A9r2tQpjQe_",
    "outputId": "fa0f90bd-bf8a-470d-c87f-c310763bb36a"
   },
   "outputs": [],
   "source": [
    "plt.plot(mae_vec)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "x1ejKoyDLWoL",
    "outputId": "d92ba717-bfd1-482b-e644-ac14085d6ce8"
   },
   "outputs": [],
   "source": [
    "plt.plot(accuracy_vec)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy_vec')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KjDvwNTgOWzE",
    "outputId": "c2140ce0-3f77-49ca-cc8f-7ed5ae8729a4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WU832ZLHQMwC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7n5KyoOEQMyv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlQa4AUxQM1S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cI9D1aXclFrd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0NXCzfRUPDh7",
    "outputId": "bd503e3e-99f8-44fe-9bee-566c6cf71bbf"
   },
   "source": [
    "# Test model prediction on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "bQu1fIdgQM5G",
    "outputId": "30b7d651-bf21-46f5-be54-885ab5329678",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# _, _, real_vec, predicted_vec, confusion_matrix = validate_model_with_real_number_of_persons(loader=loader_with_real_people_count_valid, model=unet)\n",
    "\n",
    "_, _, real_vec, predicted_vec, confusion_matrix = validate_model_with_real_number_of_persons(loader=loader_with_real_people_count_test, model=unet, data_plotting_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILE50y9kS0Du",
    "outputId": "b9a03d3b-bbb8-43e0-c50c-6d0482bc9c45"
   },
   "outputs": [],
   "source": [
    "confusion_matrix\n",
    "\n",
    "txt = ''\n",
    "for i in range(confusion_matrix.shape[0]):\n",
    "    for j in range(confusion_matrix.shape[1]):\n",
    "        val = confusion_matrix[j][i]\n",
    "        txt += str(val) + '\\t'\n",
    "    txt += '\\n'\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot prediction graphs (test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "id": "7L3aCk-7QM7z",
    "outputId": "d2d8eec9-1354-44c7-8620-58e705aadfe2"
   },
   "outputs": [],
   "source": [
    "predicted_vec_round = [round(v) for v in predicted_vec]\n",
    "\n",
    "fig=plt.figure(figsize=(11,4), dpi= 150, facecolor='w', edgecolor='k')\n",
    "t = [i * 0.5 for i in range(len(predicted_vec_round))]\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(t, real_vec, '--', linewidth=2.5)\n",
    "#plt.plot(predicted_vec, linewidth=1)\n",
    "\n",
    "plt.plot(t, predicted_vec, linewidth=1)\n",
    "\n",
    "plt.ylim(-0.2, 5.75)\n",
    "plt.vlines([470/2, 470*4/2],  -0.2, 5.75, linestyle='dashed', colors='k')\n",
    "plt.xlim(left=0, right=470*5/2)\n",
    "\n",
    "# plt.title('Number of people')\n",
    "plt.legend(['ground truth', 'prediction'])\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('person count [-]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSkR7McVmjXc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyNOYKrDBd2Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "id": "2E7pMftwl7iz",
    "outputId": "143d7be5-48cf-4bbd-d69f-9fa940b8224e"
   },
   "outputs": [],
   "source": [
    "predicted_vec_round = [round(v) for v in predicted_vec]\n",
    "\n",
    "fig=plt.figure(figsize=(11,4), dpi= 150, facecolor='w', edgecolor='k')\n",
    "t = [i * 0.5 for i in range(len(predicted_vec_round))]\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(t, real_vec, '--', linewidth=2.5)\n",
    "\n",
    "#plt.plot(predicted_vec, linewidth=1)\n",
    "\n",
    "plt.ylim(-0.2, 6.1)\n",
    "plt.vlines([470/2, 470*4/2],  -0.2, 6.1, linestyle='dashed', colors='k')\n",
    "plt.xlim(left=0, right=470*5/2)\n",
    "\n",
    "\n",
    "plt.plot(t, predicted_vec_round, linewidth=1)\n",
    "\n",
    "# plt.title('Number of people')\n",
    "plt.legend(['ground truth', 'rounded prediction '])\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('person count [-]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "id": "zboRO37CQM9c",
    "outputId": "f2d2a540-b292-49d8-d7c3-d5bd637cc70f"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predicted_error_round = [real_vec[i] - predicted_vec_round[i] for i in range(len(predicted_vec_round))]\n",
    "\n",
    "prediction_error = np.array(real_vec) - np.array(predicted_vec)\n",
    "relative_error = [prediction_error[i] / max(1, real_vec[i]) for i in range(len(real_vec))]\n",
    "\n",
    "fig=plt.figure(figsize=(11,4), dpi= 150, facecolor='w', edgecolor='k')\n",
    "t = [i * 0.5 for i in range(len(predicted_vec_round))]\n",
    "plt.grid()\n",
    "plt.plot(t, predicted_error_round, linewidth=1)\n",
    "plt.plot(t, prediction_error, linewidth=1)\n",
    "\n",
    "plt.ylim(-2.3, 2.1)\n",
    "plt.vlines([470/2, 470*4/2],  -0.2, 6.1, linestyle='dashed', colors='k')\n",
    "plt.xlim(left=0, right=470*5/2)\n",
    "\n",
    "plt.legend(['rounded prediction error', 'prediction error'])\n",
    "\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('person count prediction error [-]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOo5Fj6-PfJ0"
   },
   "outputs": [],
   "source": [
    "# optionally - save the model\n",
    "\n",
    "# unet_cpu = unet.to(cpu_device)\n",
    "# torch.save(unet_cpu.state_dict(), 'model2')\n",
    "# unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9TU63GcPfMS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQUvs0IWcwjA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvRZ3iYNcwly"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot example frame from raw IR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vSeyTf83cwoc",
    "outputId": "b1ba8750-3d16-47de-f937-620bb4f01960"
   },
   "outputs": [],
   "source": [
    "ir_frame = np.array([[24.92, 26.03, 24.15, 24.76, 25.48, 27.32, 27.36, 26.77, 25.12,\n",
    "        25.47, 24.15, 24.28, 26.81, 28.4 , 28.01, 28.71, 28.57, 28.56,\n",
    "        26.56, 25.54, 23.49, 23.73, 22.84, 22.81, 23.02, 23.32, 23.05,\n",
    "        22.93, 22.48, 23.81, 23.06, 23.18],\n",
    "       [26.87, 25.44, 24.83, 25.28, 26.23, 27.69, 28.57, 28.07, 27.63,\n",
    "        27.38, 25.5 , 24.76, 26.29, 28.1 , 28.13, 28.02, 28.46, 27.89,\n",
    "        27.  , 26.71, 24.43, 23.7 , 23.15, 22.89, 23.67, 23.69, 22.92,\n",
    "        22.82, 23.99, 24.24, 23.25, 23.47],\n",
    "       [23.72, 24.56, 23.98, 24.44, 24.67, 26.34, 26.76, 27.65, 28.08,\n",
    "        27.69, 24.86, 25.58, 25.84, 26.9 , 27.28, 27.23, 27.38, 27.26,\n",
    "        25.97, 25.48, 23.21, 23.34, 22.8 , 23.53, 23.91, 23.98, 23.49,\n",
    "        23.79, 23.44, 24.18, 22.76, 23.64],\n",
    "       [25.08, 24.97, 24.8 , 24.41, 24.52, 25.08, 26.24, 26.34, 27.85,\n",
    "        27.4 , 25.92, 24.55, 25.4 , 26.22, 27.  , 26.78, 26.48, 25.62,\n",
    "        24.28, 23.67, 23.63, 23.66, 23.08, 23.24, 24.84, 24.73, 24.08,\n",
    "        23.97, 23.46, 24.48, 23.48, 23.42],\n",
    "       [24.29, 24.68, 23.43, 24.22, 24.41, 24.78, 24.37, 25.16, 28.24,\n",
    "        27.88, 25.88, 27.26, 24.42, 25.76, 26.6 , 26.69, 25.35, 24.22,\n",
    "        23.34, 23.54, 23.12, 23.5 , 22.87, 23.21, 23.77, 24.03, 22.81,\n",
    "        23.36, 23.07, 23.86, 22.84, 23.41],\n",
    "       [24.94, 24.44, 24.64, 24.14, 26.21, 26.99, 25.45, 25.62, 29.93,\n",
    "        27.7 , 28.21, 26.01, 27.03, 25.55, 27.24, 26.03, 25.41, 24.36,\n",
    "        23.88, 23.68, 23.59, 23.43, 23.44, 23.11, 23.19, 23.44, 22.74,\n",
    "        23.21, 23.5 , 23.86, 23.29, 23.17],\n",
    "       [23.91, 24.39, 23.67, 24.53, 27.17, 28.45, 24.57, 24.15, 24.82,\n",
    "        25.1 , 27.73, 29.65, 26.85, 29.25, 25.81, 25.55, 23.88, 23.79,\n",
    "        23.53, 23.59, 23.22, 23.34, 23.15, 23.36, 22.85, 23.14, 22.63,\n",
    "        23.43, 23.01, 23.52, 22.7 , 23.4 ],\n",
    "       [24.87, 24.42, 24.75, 24.55, 25.5 , 25.9 , 24.19, 24.07, 24.11,\n",
    "        25.8 , 28.89, 28.46, 29.9 , 28.31, 27.03, 25.82, 23.87, 24.08,\n",
    "        23.9 , 23.77, 23.72, 23.64, 23.54, 23.15, 23.2 , 23.14, 23.04,\n",
    "        23.13, 22.96, 23.34, 23.07, 23.13],\n",
    "       [24.35, 24.58, 24.04, 24.59, 23.9 , 24.13, 23.28, 23.63, 23.9 ,\n",
    "        24.83, 28.4 , 28.89, 29.88, 27.84, 27.97, 24.5 , 24.09, 24.18,\n",
    "        23.35, 23.62, 23.49, 23.54, 23.02, 23.55, 23.12, 23.38, 22.66,\n",
    "        23.16, 22.84, 23.39, 22.82, 23.27],\n",
    "       [25.06, 25.57, 26.22, 26.  , 24.57, 24.42, 23.73, 23.44, 23.92,\n",
    "        25.16, 27.18, 29.98, 26.84, 29.15, 25.89, 25.81, 23.84, 23.65,\n",
    "        23.94, 23.61, 23.57, 23.69, 23.56, 23.19, 23.26, 23.31, 23.47,\n",
    "        23.25, 23.27, 23.4 , 23.14, 23.69],\n",
    "       [24.35, 24.41, 24.39, 24.14, 23.74, 23.64, 23.63, 23.63, 23.47,\n",
    "        23.29, 26.78, 23.66, 27.  , 24.  , 25.18, 24.02, 24.  , 23.73,\n",
    "        23.55, 23.83, 23.43, 23.88, 23.33, 23.57, 23.09, 23.49, 23.  ,\n",
    "        23.35, 23.34, 24.03, 23.67, 24.16],\n",
    "       [24.61, 24.81, 24.15, 23.93, 24.46, 24.1 , 24.25, 23.87, 23.69,\n",
    "        24.21, 23.79, 25.2 , 23.9 , 25.06, 24.36, 24.  , 23.92, 23.99,\n",
    "        23.9 , 23.77, 23.65, 23.87, 23.72, 23.49, 23.2 , 23.2 , 22.74,\n",
    "        23.13, 23.77, 24.07, 24.13, 23.46],\n",
    "       [24.09, 24.12, 24.31, 23.84, 24.05, 24.1 , 23.95, 24.04, 23.7 ,\n",
    "        23.72, 23.25, 23.45, 23.73, 23.91, 24.25, 24.06, 23.99, 24.02,\n",
    "        23.65, 24.03, 23.78, 24.19, 23.6 , 24.  , 23.07, 23.33, 22.88,\n",
    "        23.67, 24.01, 24.63, 25.33, 24.86],\n",
    "       [25.21, 25.31, 24.46, 24.29, 24.48, 24.74, 25.13, 24.63, 24.05,\n",
    "        24.17, 24.05, 23.89, 24.07, 24.15, 24.11, 23.85, 24.02, 24.07,\n",
    "        24.24, 24.06, 24.37, 24.31, 23.81, 23.86, 23.53, 23.24, 23.78,\n",
    "        24.04, 24.82, 24.7 , 25.37, 25.25],\n",
    "       [24.86, 24.43, 24.18, 24.34, 24.18, 24.85, 26.59, 28.64, 25.67,\n",
    "        24.7 , 24.33, 24.51, 23.94, 24.14, 24.2 , 24.16, 24.13, 24.28,\n",
    "        24.13, 24.36, 24.27, 24.38, 23.88, 23.97, 23.37, 23.59, 24.82,\n",
    "        28.25, 25.72, 25.77, 24.01, 24.46],\n",
    "       [25.4 , 25.  , 25.06, 24.46, 25.01, 25.07, 25.41, 25.96, 24.75,\n",
    "        24.99, 24.83, 24.9 , 24.66, 24.45, 24.58, 24.15, 24.63, 24.47,\n",
    "        24.57, 24.18, 24.51, 24.59, 24.11, 24.2 , 24.06, 23.76, 24.73,\n",
    "        25.52, 25.24, 24.99, 24.37, 24.07],\n",
    "       [25.03, 24.6 , 24.43, 24.55, 26.85, 27.28, 25.86, 25.75, 24.17,\n",
    "        24.73, 25.01, 25.23, 24.65, 24.88, 24.46, 25.03, 24.63, 24.75,\n",
    "        24.52, 24.66, 24.56, 24.74, 24.13, 24.06, 23.71, 24.25, 25.18,\n",
    "        25.58, 25.35, 25.47, 24.25, 24.64],\n",
    "       [25.37, 25.37, 25.57, 25.4 , 28.33, 28.9 , 27.46, 26.55, 25.14,\n",
    "        25.23, 25.4 , 25.29, 25.23, 24.94, 25.14, 24.88, 25.19, 24.92,\n",
    "        25.08, 24.65, 24.78, 25.03, 24.73, 24.3 , 24.66, 25.24, 26.23,\n",
    "        26.25, 26.72, 26.8 , 25.13, 24.64],\n",
    "       [24.89, 24.85, 24.97, 24.95, 27.41, 27.94, 27.82, 27.49, 24.66,\n",
    "        24.95, 24.55, 25.12, 24.78, 24.76, 24.44, 24.8 , 24.55, 24.72,\n",
    "        24.66, 24.77, 24.33, 24.9 , 24.74, 24.95, 24.31, 25.36, 25.94,\n",
    "        26.79, 27.76, 28.84, 25.35, 25.43],\n",
    "       [25.48, 25.23, 25.37, 24.99, 26.87, 27.71, 28.55, 27.02, 25.34,\n",
    "        25.07, 24.91, 24.62, 24.86, 24.86, 24.81, 24.84, 24.71, 24.56,\n",
    "        24.6 , 24.58, 24.86, 24.48, 25.01, 24.75, 24.54, 24.98, 25.93,\n",
    "        26.78, 26.92, 26.99, 25.18, 24.89],\n",
    "       [25.51, 24.57, 24.97, 24.85, 25.98, 26.69, 26.61, 25.87, 24.41,\n",
    "        24.79, 24.35, 24.35, 24.11, 24.5 , 24.16, 24.31, 24.23, 24.16,\n",
    "        24.09, 24.08, 24.21, 24.51, 24.16, 24.91, 24.62, 24.79, 24.74,\n",
    "        25.91, 25.02, 25.07, 24.09, 24.79],\n",
    "       [25.94, 25.45, 25.77, 25.37, 26.86, 27.43, 27.79, 26.72, 25.13,\n",
    "        24.65, 24.83, 24.3 , 24.62, 24.45, 24.59, 24.07, 24.4 , 24.12,\n",
    "        24.12, 23.81, 24.17, 24.15, 24.71, 24.39, 24.48, 24.54, 24.81,\n",
    "        24.31, 24.9 , 24.83, 25.02, 24.33],\n",
    "       [27.3 , 25.43, 25.73, 26.34, 27.5 , 28.89, 28.77, 28.7 , 27.52,\n",
    "        27.18, 25.55, 25.42, 24.73, 24.87, 24.42, 24.21, 24.32, 23.97,\n",
    "        23.67, 24.11, 23.77, 24.4 , 24.78, 25.29, 25.47, 25.95, 25.32,\n",
    "        25.33, 24.28, 24.62, 25.11, 24.76],\n",
    "       [28.99, 27.23, 27.41, 26.89, 28.01, 28.28, 29.35, 28.76, 28.5 ,\n",
    "        27.6 , 26.96, 26.8 , 26.83, 26.22, 26.  , 24.83, 24.03, 23.92,\n",
    "        23.81, 23.57, 24.  , 24.14, 25.55, 26.06, 26.67, 26.77, 26.19,\n",
    "        25.02, 24.79, 24.84, 24.02, 23.8 ]])\n",
    "\n",
    "# frame 416 from video '05_05_2021__0to5_people/004__13_10_20'\n",
    "centre_points = [\n",
    "        [\n",
    "          16.125,\n",
    "          0.9375\n",
    "        ],\n",
    "        [\n",
    "          28.1875,\n",
    "          18.6875\n",
    "        ],\n",
    "        [\n",
    "          7.6875,\n",
    "          2.25\n",
    "        ],\n",
    "        [\n",
    "          6.375,\n",
    "          18.125\n",
    "        ],\n",
    "        [\n",
    "          12.75,\n",
    "          7.9375\n",
    "        ]\n",
    "      ]\n",
    "\n",
    "\n",
    "flipped_frame = np.flip(ir_frame, 1) \n",
    "\n",
    "plt.imshow(flipped_frame)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig=plt.figure(figsize=(6,4), dpi= 150, facecolor='w', edgecolor='k')\n",
    "plt.imshow(flipped_frame, 'inferno'); cbar = plt.colorbar(); cbar.set_label('Temperature [°C]')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f'{\"#\"*60} \\n Reconstructed frame (training data):')\n",
    "fig=plt.figure(figsize=(6,4), dpi= 150, facecolor='w', edgecolor='k')\n",
    "recontructed_image = get_img_reconstructed_from_labels(centre_points)\n",
    "recontructed_image_flipped = np.flip(recontructed_image, 1) \n",
    "plt.imshow(recontructed_image_flipped)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "3ntzLRCIeab6",
    "outputId": "a88c84ce-8e5d-4b34-ebc8-298ea93f89a3"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOY83R7oby5i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKmRkXVQby8B"
   },
   "source": [
    "# Comparison with the fully connected network from another article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ONBlASXbwiv",
    "outputId": "d1a95909-d99c-44c0-a06b-bd3b3c8a3b34"
   },
   "outputs": [],
   "source": [
    "class FNNClassifier(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=768, out_features=512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=6),\n",
    "            )\n",
    "        \n",
    "\n",
    "model_fnn = FNNClassifier().double()\n",
    "model_fnn.to(device)\n",
    "print('Number of patameters:', sum(p.numel() for p in model_fnn.parameters() if p.requires_grad))\n",
    "\n",
    "\n",
    "class IrPersonsDatasetForFnn(torch.utils.data.Dataset):\n",
    "    def __init__(self, augmented_data: AugmentedBatchesTrainingData, transform=None):\n",
    "        self.augmented_data = AugmentedBatchesTrainingData\n",
    "        self.transform = transform\n",
    "        self._index_to_batch_and_subindex_map = {}\n",
    "        \n",
    "        i = 0\n",
    "        for batch in augmented_data.batches:\n",
    "            for j in range(len(batch.raw_ir_data)):\n",
    "                self._index_to_batch_and_subindex_map[i] = (batch, j) \n",
    "                i += 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len (self._index_to_batch_and_subindex_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            raise Exception(\"Not supported\")\n",
    "            #idx = idx.tolist()\n",
    "\n",
    "        batch, subindex = self._index_to_batch_and_subindex_map[idx]\n",
    "        frame = batch.normalized_ir_data[subindex][np.newaxis, :, :]\n",
    "        return frame, len(batch.centre_points[subindex])\n",
    "\n",
    "\n",
    "training_dataset_fnn = IrPersonsDatasetForFnn(augmented_data_training)\n",
    "validation_dataset_fnn = IrPersonsDatasetForFnn(augmented_data_validation)\n",
    "test_dataset_fnn = IrPersonsDatasetForFnn(augmented_data_test)\n",
    "\n",
    "\n",
    "trainloader_fnn = torch.utils.data.DataLoader(training_dataset_fnn, batch_size=16, shuffle=True)\n",
    "valloader_fnn = torch.utils.data.DataLoader(validation_dataset_fnn, batch_size=16, shuffle=False)\n",
    "testloader_fnn = torch.utils.data.DataLoader(test_dataset_fnn, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "xyh7eP0bgQSe",
    "outputId": "5632ea37-a25d-446f-8a10-7796c7c5f0d6"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader_fnn)\n",
    "ir_frmaes, labels = dataiter.next()\n",
    "\n",
    "ir_frame_normalized_0 = ir_frmaes[0].numpy().squeeze()\n",
    "plt.imshow(ir_frame_normalized_0)\n",
    "print(f'Persons: {labels[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRM19K8QgQVp",
    "outputId": "3f54dab6-095b-4ca0-b964-640eb6e9dee7"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFkBjKrh5R0M",
    "outputId": "713a0f06-0cc5-45cd-c022-b9d80c963b7b"
   },
   "outputs": [],
   "source": [
    "def validate_model_with_real_number_of_persons_fnn(loader, model, data_plotting_interval=3000, skip_confusion_matrix=False, loss_fn=None):\n",
    "    correct_count = 0\n",
    "    tested_frames = 0\n",
    "    number_of_frames_with_n_persons = {}\n",
    "    number_of_frames_with_n_persons_predicted_correctly = {}\n",
    "\n",
    "    MAX_PEOPLE_COUNT = 5\n",
    "    confusion_matrix = np.zeros(shape=(MAX_PEOPLE_COUNT+1, MAX_PEOPLE_COUNT+1), dtype=int)\n",
    "\n",
    "    mae_rounded_sum = 0\n",
    "    mse_rounded_sum = 0\n",
    "    \n",
    "    vec_real_number_of_persons = []\n",
    "    vec_predicted_number_of_persons = []\n",
    "\n",
    "    running_loss = 0\n",
    "    steps = 0\n",
    "\n",
    "    for frame, labels in loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(frame.to(device))\n",
    "            if loss_fn:\n",
    "                loss = loss_fn(outputs, labels.to(device))\n",
    "                running_loss += loss.item()\n",
    "            outputs = outputs.to(cpu_device)\n",
    "            steps += 1\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            predicted_vals = outputs[i].numpy()\n",
    "\n",
    "            pred_label = np.argmax(predicted_vals)\n",
    "            true_label = labels.numpy()[i]\n",
    "\n",
    "            if not skip_confusion_matrix:\n",
    "                confusion_matrix[true_label][pred_label] += 1\n",
    "\n",
    "            rounded_error = abs(pred_label - true_label)\n",
    "            mae_rounded_sum += rounded_error\n",
    "            mse_rounded_sum += rounded_error*rounded_error\n",
    "\n",
    "            number_of_frames_with_n_persons[pred_label] = \\\n",
    "                number_of_frames_with_n_persons.get(pred_label, 0) + 1\n",
    "\n",
    "            if true_label == pred_label:\n",
    "                correct_count += 1\n",
    "                number_of_frames_with_n_persons_predicted_correctly[pred_label] = \\\n",
    "                    number_of_frames_with_n_persons_predicted_correctly.get(pred_label, 0) + 1\n",
    "\n",
    "            tested_frames += 1\n",
    "            \n",
    "            vec_real_number_of_persons.append(true_label)\n",
    "            vec_predicted_number_of_persons.append(pred_label)\n",
    "            \n",
    "\n",
    "    mae_rounded = mae_rounded_sum / tested_frames\n",
    "    mse_rounded = mse_rounded_sum / tested_frames\n",
    "\n",
    "    model_accuracy = correct_count / tested_frames\n",
    "\n",
    "    total_loss = running_loss / steps\n",
    "    \n",
    "    print(f\"Number of tested frames: {tested_frames}\")\n",
    "    print(f\"Model Accuracy = {model_accuracy}\")\n",
    "    print('Predicted:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons.items()]))\n",
    "    print('Predicted correctly:\\n' + '\\n'.join([f'   {count} frames with {no} persons' for no, count in number_of_frames_with_n_persons_predicted_correctly.items()]))\n",
    "    print(f'mae_rounded: {mae_rounded}')\n",
    "    print(f'mse_rounded: {mse_rounded}')\n",
    "    print(f'total_loss: {total_loss}')\n",
    "    \n",
    "    \n",
    "    return model_accuracy, mae_rounded, vec_real_number_of_persons, vec_predicted_number_of_persons, confusion_matrix, total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbLhc_dWgQYf",
    "outputId": "7595181b-991a-4f42-940a-a7bd11353b34"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.0001, momentum=0.9)\n",
    "time0 = time.time()\n",
    "epochs = 200\n",
    "# criterion = nn.NLLLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "mae_vec = []\n",
    "accuracy_vec = []\n",
    "\n",
    "\n",
    "best_mae = None\n",
    "best_mae_model = None\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    model_fnn.train(True)\n",
    "    for images, labels in trainloader_fnn:    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model_fnn(images.to(device))\n",
    "        loss = criterion(output, labels.to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "    epoch_loss = running_loss / step\n",
    "    train_loss.append(epoch_loss)\n",
    "    print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader_fnn)))\n",
    "    print(\"\\nTraining Time (in minutes) =\",(time.time()-time0)/60)\n",
    "\n",
    "\n",
    "    model_fnn.train(False)\n",
    "    accuracy, mae, _, _, _, loss = validate_model_with_real_number_of_persons_fnn(loader=valloader_fnn, model=model_fnn, skip_confusion_matrix=True, loss_fn=criterion)\n",
    "\n",
    "    if best_mae is None or mae < best_mae:\n",
    "        print('New best MAE model!')\n",
    "        best_mae = mae\n",
    "        best_mae_model = copy.deepcopy(model_fnn)\n",
    "    \n",
    "    valid_loss.append(loss)\n",
    "    mae_vec.append(mae)\n",
    "    accuracy_vec.append(accuracy)\n",
    "\n",
    "model_fnn = best_mae_model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KnNJz-lo-Xqw",
    "outputId": "afb1f7cc-ed4a-428f-9cff-2709ec6bf7b1"
   },
   "outputs": [],
   "source": [
    "validate_model_with_real_number_of_persons(loader=testloader_fnn, model=model_fnn, skip_confusion_matrix=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "image_segmentation_unet_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
